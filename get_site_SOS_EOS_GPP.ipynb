{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ee7eb6-2f4d-4c77-8d42-ed6439cab9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, fnmatch\n",
    "import shutil\n",
    "import zipfile\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # non-GUI backend for script-only plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "import ruptures as rpt\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.interpolate import splrep, PPoly\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb291ab-c41a-4442-ba64-f4f2e007f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### setting\n",
    "dir_flux = \"xxx\"\n",
    "fn_fluxlist = \"fluxsite_info_metalist.xlsx\"\n",
    "dir_csv_out = \"xxx\"\n",
    "dir_savefig = \"xxx\"\n",
    "\n",
    "window_size = 13\n",
    "poly_deg1 = 2\n",
    "threshold_30 = 0.5 # 0.3 \n",
    "penalty = 0.5\n",
    "\n",
    "# columns in FLUXNET, AmeriFlux, ICOS\n",
    "pick_cols = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"P_F\", \"P_F_QC\", \"SW_IN_F\", \"SW_IN_F_QC\",\n",
    "             \"GPP_DT_VUT_REF\", \"NEE_VUT_REF_QC\", \"TA_F\", \"TA_F_QC\", \"USTAR\"]\n",
    "dic_QC = {\"P_F\":\"P_F_QC\", \"TA_F\":\"TA_F_QC\", \"SW_IN_F\":\"SW_IN_F_QC\", \n",
    "          \"GPP_DT_VUT_REF\":\"NEE_VUT_REF_QC\"}\n",
    "dic_QC_flux = {\"GPP_DT_VUT_REF\":\"NEE_VUT_REF_QC\"}\n",
    "var_to_QC = [\"P_F\", \"TA_F\", \"SW_IN_F\", \"GPP_DT_VUT_REF\"]\n",
    "\n",
    "# columns in Ozflux\n",
    "pick_cols_Ozflux = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"P_F\", \"P_F_QC\", \"SW_IN_F\", \"SW_IN_F_QC\", \n",
    "                    \"TA_F\", \"TA_F_QC\", \"GPP\", \"GPP_QC\", \"USTAR\"]\n",
    "dic_QC_Ozflux = {\"P_F\":\"P_F_QC\", \"TA_F\":\"TA_F_QC\", \"SW_IN_F\":\"SW_IN_F_QC\",\n",
    "                 \"USTAR\":\"USTAR_QC\", \"GPP\":\"GPP_QC\"}\n",
    "dic_QC_Ozflux_flux = {\"GPP\":\"GPP_QC\"}\n",
    "var_to_QC_Ozflux = [\"P_F\", \"TA_F\", \"SW_IN_F\", \"USTAR\", \"GPP\"]\n",
    "\n",
    "# columns in LBA-ECO\n",
    "pick_cols_LBAECO = [\"TIMESTAMP_END\", \"USTAR\", \"GPP\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12af6943-9bad-4679-92b3-c1ae8e143935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total site =  177\n",
      "Index(['site_ID', 'SITE_NAME', 'LOCATION_LAT', 'LOCATION_LONG',\n",
      "       'LOCATION_ELEV', 'IGBP', 'year_start', 'year_end', 'measure_h',\n",
      "       'measure_h_year', 'canopy_h', 'canopy_h_year', 'source', 'Unnamed: 13'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### filter flux site\n",
    "df_sitelist = pd.read_excel(dir_flux+fn_fluxlist)\n",
    "forest_site_mask = (((df_sitelist[\"IGBP\"]==\"EBF\") | (df_sitelist[\"IGBP\"]==\"ENF\") | \n",
    "                     (df_sitelist[\"IGBP\"]==\"DBF\") | (df_sitelist[\"IGBP\"]==\"DNF\") | \n",
    "                     (df_sitelist[\"IGBP\"]==\"MF\")) & \n",
    "                    (df_sitelist[\"year_end\"]-df_sitelist[\"year_start\"]+1 >= 3))\n",
    "df_forest_sitelist = df_sitelist[forest_site_mask].reindex()\n",
    "total_site = len(df_forest_sitelist[\"site_ID\"])\n",
    "print(\"total site = \", total_site)\n",
    "print(df_forest_sitelist.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5433215f-55a4-4492-9315-539b1cfed279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_savgol(dataframe, column_name):\n",
    "    x = dataframe[column_name].values\n",
    "    x_smooth = savgol_filter(x, window_size*2, poly_deg1)\n",
    "    x_diff = x/x_smooth\n",
    "    x_diff_cri1 = np.nanmean(x_diff)+np.nanstd(x_diff)\n",
    "    x_diff_cri2 = np.nanmean(x_diff)-np.nanstd(x_diff)\n",
    "    x_out = np.where(((x_diff>x_diff_cri1) | (x_diff<x_diff_cri2)), x_smooth, x)\n",
    "    dataframe[column_name] = x_out\n",
    "    return dataframe\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145e50b9-0551-4787-9d16-6641b6c0da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_SOS_EOS(dataframe, column_name):\n",
    "    try:\n",
    "        SOS_EOS_DOY = np.empty([4])\n",
    "        SOS_EOS_DOY[:] = np.nan # SOS1, EOS1, SOS2, EOS2\n",
    "        SOS_EOS_intersect = np.empty([4])\n",
    "        SOS_EOS_intersect[:] = np.nan # SOS1, EOS1, SOS2, EOS2\n",
    "        LOS = np.empty([1])\n",
    "        LOS[:] = np.nan\n",
    "        \n",
    "        # detect change points\n",
    "        x1 = dataframe[column_name].values\n",
    "        algo = rpt.Pelt(model=\"l2\", min_size=window_size)\n",
    "        algo.fit(x1)\n",
    "        x1_result = algo.predict(pen=penalty)\n",
    "        \n",
    "        # take avg between change points\n",
    "        x1_avg = np.empty(np.shape(x1_result))\n",
    "        x1_avg[:] = np.nan\n",
    "        x1_bkpt = np.append([0],x1_result)\n",
    "        for it in range(len(x1_avg)):\n",
    "            x1_avg[it] = np.nanmean(x1[x1_bkpt[it]:x1_bkpt[it+1]+1])\n",
    "        x1_lmax = argrelextrema(x1_avg, np.greater) # get the position of local max\n",
    "        peak_count = np.nansum(np.where((np.array(x1_result)[x1_lmax[0].astype(int)]>=365) & (np.array(x1_result)[x1_lmax[0].astype(int)]<365*2-1), 1, 0))\n",
    "        x1_lmin = argrelextrema(x1_avg, np.less) # get the position of local min\n",
    "        trough_count = np.nansum(np.where((np.array(x1_result)[x1_lmin[0].astype(int)]>=365) & (np.array(x1_result)[x1_lmin[0].astype(int)]<365*2-1), 1, 0))\n",
    "    \n",
    "        if ((peak_count==1) & (trough_count==1)):\n",
    "            # find local min/max values and SOS/EOS between local min/max\n",
    "            x1_lmax_valid = np.array(x1_lmax[0])[((np.array(x1_result)[x1_lmax[0].astype(int)]>=365) & (np.array(x1_result)[x1_lmax[0].astype(int)]<365*2-1))]\n",
    "            x1_lmax_value1 = np.nanmax(x1[x1_result[x1_lmax_valid[0]-1]:x1_result[x1_lmax_valid[0]]])\n",
    "            x1_lmax_pos1 = np.nanargmax(x1[x1_result[x1_lmax_valid[0]-1]:x1_result[x1_lmax_valid[0]]])+x1_result[x1_lmax_valid[0]-1]\n",
    "            \n",
    "            x1_lmin_valid = np.array(x1_lmin[0])[((np.array(x1_result)[x1_lmin[0].astype(int)]>=365) & (np.array(x1_result)[x1_lmin[0].astype(int)]<365*2-1))]\n",
    "            x1_lmin_value1 = np.nanmin(x1[x1_result[x1_lmin_valid[0]-1]:x1_result[x1_lmin_valid[0]]])\n",
    "            x1_lmin_pos1 = np.nanargmin(x1[x1_result[x1_lmin_valid[0]-1]:x1_result[x1_lmin_valid[0]]])+x1_result[x1_lmin_valid[0]-1]\n",
    "    \n",
    "            # determine the amplitude and the intersection for SOS and EOS\n",
    "            if (x1_lmin_valid[0]<x1_lmax_valid[0]):\n",
    "                SOS_intercect1 = x1_lmin_value1+abs(x1_lmax_value1-x1_lmin_value1)*threshold_30\n",
    "                EOS_intercect1 = x1_lmax_value1-abs(x1_lmax_value1-x1_lmin_value1)*(1-threshold_30)\n",
    "                SOS_EOS_intersect[0] = SOS_intercect1\n",
    "                SOS_EOS_intersect[1] = EOS_intercect1\n",
    "                SOS_EOS_intersect[2] = np.nan\n",
    "                SOS_EOS_intersect[3] = np.nan\n",
    "            \n",
    "                # find the root for SOS and EOS (should lie between min and max index)\n",
    "                tck_x = dataframe.index.values[x1_lmin_pos1:x1_lmax_pos1+1]+1\n",
    "                tck_y = x1[x1_lmin_pos1:x1_lmax_pos1+1]\n",
    "                tck1 = splrep(tck_x, tck_y-SOS_intercect1, s=0)\n",
    "                ppoly1 = PPoly.from_spline(tck1)\n",
    "                ppoly_r1 = ppoly1.roots(extrapolate=False)\n",
    "                if (len(ppoly_r1)>1):\n",
    "                    SOS_EOS_DOY[0] = ppoly_r1[-1]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[0] = ppoly_r1\n",
    "                \n",
    "                tck_x = dataframe.index.values[x1_lmax_pos1:x1_lmin_pos1+365+1]+1\n",
    "                tck_y = x1[x1_lmax_pos1:x1_lmin_pos1+365+1]\n",
    "                tck2 = splrep(tck_x, tck_y-EOS_intercect1, s=0)\n",
    "                ppoly2 = PPoly.from_spline(tck2)\n",
    "                ppoly_r2 = ppoly2.roots(extrapolate=False)\n",
    "                if (len(ppoly_r2)>1):\n",
    "                    SOS_EOS_DOY[1] = ppoly_r2[0]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[1] = ppoly_r2\n",
    "                \n",
    "                SOS_EOS_DOY[2] = np.nan\n",
    "                SOS_EOS_DOY[3] = np.nan\n",
    "                # LOS[0] = SOS_EOS_DOY[1]-SOS_EOS_DOY[0]\n",
    "                \n",
    "            else:\n",
    "                EOS_intercect1 = x1_lmax_value1-abs(x1_lmax_value1-x1_lmin_value1)*(1-threshold_30)\n",
    "                SOS_intercect1 = x1_lmin_value1+abs(x1_lmax_value1-x1_lmin_value1)*threshold_30\n",
    "                SOS_EOS_intersect[0] = SOS_intercect1\n",
    "                SOS_EOS_intersect[1] = EOS_intercect1\n",
    "                SOS_EOS_intersect[2] = np.nan\n",
    "                SOS_EOS_intersect[3] = np.nan\n",
    "            \n",
    "                # find the root for SOS and EOS (should lie between min and max index)\n",
    "                tck_x = dataframe.index.values[x1_lmax_pos1:x1_lmin_pos1+1]+1\n",
    "                tck_y = x1[x1_lmax_pos1:x1_lmin_pos1+1]\n",
    "                tck1 = splrep(tck_x, tck_y-EOS_intercect1, s=0)\n",
    "                ppoly1 = PPoly.from_spline(tck1)\n",
    "                ppoly_r1 = ppoly1.roots(extrapolate=False)\n",
    "                if (len(ppoly_r1)>1):\n",
    "                    SOS_EOS_DOY[1] = ppoly_r1[0]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[1] = ppoly_r1\n",
    "            \n",
    "                tck_x = dataframe.index.values[x1_lmin_pos1:x1_lmax_pos1+365+1]+1\n",
    "                tck_y = x1[x1_lmin_pos1:x1_lmax_pos1+365+1]\n",
    "                tck2 = splrep(tck_x, tck_y-SOS_intercect1, s=0)\n",
    "                ppoly2 = PPoly.from_spline(tck2)\n",
    "                ppoly_r2 = ppoly2.roots(extrapolate=False)\n",
    "                if (len(ppoly_r2)>1):\n",
    "                    SOS_EOS_DOY[0] = ppoly_r2[-1]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[0] = ppoly_r2\n",
    "    \n",
    "                SOS_EOS_DOY[2] = np.nan\n",
    "                SOS_EOS_DOY[3] = np.nan\n",
    "                # LOS[0] = (SOS_EOS_DOY[1]-(SOS_EOS_DOY[0]-365))\n",
    "\n",
    "        elif ((peak_count==2) & (trough_count==2)):\n",
    "            # find local min/max values and SOS/EOS between local min/max\n",
    "            x1_lmax_valid = np.array(x1_lmax[0])[((np.array(x1_result)[x1_lmax[0].astype(int)]>=365) & (np.array(x1_result)[x1_lmax[0].astype(int)]<365*2-1))]\n",
    "            x1_lmax_value1 = np.nanmax(x1[x1_result[x1_lmax_valid[0]-1]:x1_result[x1_lmax_valid[0]]])\n",
    "            x1_lmax_pos1 = np.nanargmax(x1[x1_result[x1_lmax_valid[0]-1]:x1_result[x1_lmax_valid[0]]])+x1_result[x1_lmax_valid[0]-1]\n",
    "            x1_lmax_value2 = np.nanmax(x1[x1_result[x1_lmax_valid[1]-1]:x1_result[x1_lmax_valid[1]]])\n",
    "            x1_lmax_pos2 = np.nanargmax(x1[x1_result[x1_lmax_valid[1]-1]:x1_result[x1_lmax_valid[1]]])+x1_result[x1_lmax_valid[1]-1]\n",
    "            \n",
    "            x1_lmin_valid = np.array(x1_lmin[0])[((np.array(x1_result)[x1_lmin[0].astype(int)]>=365) & (np.array(x1_result)[x1_lmin[0].astype(int)]<365*2-1))]\n",
    "            x1_lmin_value1 = np.nanmin(x1[x1_result[x1_lmin_valid[0]-1]:x1_result[x1_lmin_valid[0]]])\n",
    "            x1_lmin_pos1 = np.nanargmin(x1[x1_result[x1_lmin_valid[0]-1]:x1_result[x1_lmin_valid[0]]])+x1_result[x1_lmin_valid[0]-1]\n",
    "            x1_lmin_value2 = np.nanmin(x1[x1_result[x1_lmin_valid[1]-1]:x1_result[x1_lmin_valid[1]]])\n",
    "            x1_lmin_pos2 = np.nanargmin(x1[x1_result[x1_lmin_valid[1]-1]:x1_result[x1_lmin_valid[1]]])+x1_result[x1_lmin_valid[1]-1]\n",
    "            \n",
    "            # determine the amplitude and the intersection for SOS and EOS\n",
    "            if (x1_lmin_valid[0]<x1_lmax_valid[0]):\n",
    "                SOS_intercect1 = x1_lmin_value1+abs(x1_lmax_value1-x1_lmin_value1)*threshold_30\n",
    "                EOS_intercect1 = x1_lmax_value1-abs(x1_lmax_value1-x1_lmin_value2)*(1-threshold_30)\n",
    "                SOS_intercect2 = x1_lmin_value2+abs(x1_lmax_value2-x1_lmin_value2)*threshold_30\n",
    "                EOS_intercect2 = x1_lmax_value2-abs(x1_lmax_value2-x1_lmin_value1)*(1-threshold_30)\n",
    "                SOS_EOS_intersect[0] = SOS_intercect1\n",
    "                SOS_EOS_intersect[1] = EOS_intercect1\n",
    "                SOS_EOS_intersect[2] = SOS_intercect2\n",
    "                SOS_EOS_intersect[3] = EOS_intercect2\n",
    "                \n",
    "                # find the root for SOS and EOS (should lie between min and max index)\n",
    "                tck_x = dataframe.index.values[x1_lmin_pos1:x1_lmax_pos1+1]+1\n",
    "                tck_y = x1[x1_lmin_pos1:x1_lmax_pos1+1]\n",
    "                tck1 = splrep(tck_x, tck_y-SOS_intercect1, s=0)\n",
    "                ppoly1 = PPoly.from_spline(tck1)\n",
    "                ppoly_r1 = ppoly1.roots(extrapolate=False)\n",
    "                if (len(ppoly_r1)>1):\n",
    "                    SOS_EOS_DOY[0] = ppoly_r1[0]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[0] = ppoly_r1\n",
    "                \n",
    "                tck_x = dataframe.index.values[x1_lmax_pos1:x1_lmin_pos2+1]+1\n",
    "                tck_y = x1[x1_lmax_pos1:x1_lmin_pos2+1]\n",
    "                tck2 = splrep(tck_x, tck_y-EOS_intercect1, s=0)\n",
    "                ppoly2 = PPoly.from_spline(tck2)\n",
    "                ppoly_r2 = ppoly2.roots(extrapolate=False)\n",
    "                if (len(ppoly_r2)>1):\n",
    "                    SOS_EOS_DOY[1] = ppoly_r2[-1]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[1] = ppoly_r2\n",
    "    \n",
    "                tck_x = dataframe.index.values[x1_lmin_pos2:x1_lmax_pos2+1]+1\n",
    "                tck_y = x1[x1_lmin_pos2:x1_lmax_pos2+1]\n",
    "                tck3 = splrep(tck_x, tck_y-SOS_intercect2, s=0)\n",
    "                ppoly3 = PPoly.from_spline(tck3)\n",
    "                ppoly_r3 = ppoly3.roots(extrapolate=False)\n",
    "                if (len(ppoly_r3)>1):\n",
    "                    SOS_EOS_DOY[2] = ppoly_r3[0]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[2] = ppoly_r3\n",
    "            \n",
    "                tck_x = dataframe.index.values[x1_lmax_pos2:x1_lmin_pos1+365+1]+1\n",
    "                tck_y = x1[x1_lmax_pos2:x1_lmin_pos1+365+1]\n",
    "                tck4 = splrep(tck_x, tck_y-EOS_intercect2, s=0)\n",
    "                ppoly4 = PPoly.from_spline(tck4)\n",
    "                ppoly_r4 = ppoly4.roots(extrapolate=False)\n",
    "                if (len(ppoly_r4)>1):\n",
    "                    SOS_EOS_DOY[3] = ppoly_r4[-1]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[3] = ppoly_r4\n",
    "                # LOS[0] = (SOS_EOS_DOY[1]-SOS_EOS_DOY[0])+(SOS_EOS_DOY[3]-SOS_EOS_DOY[2])\n",
    "                \n",
    "            else:\n",
    "                EOS_intercect1 = x1_lmax_value1-abs(x1_lmax_value1-x1_lmin_value1)*(1-threshold_30)\n",
    "                SOS_intercect1 = x1_lmin_value1+abs(x1_lmax_value2-x1_lmin_value1)*threshold_30\n",
    "                EOS_intercect2 = x1_lmax_value2-abs(x1_lmax_value2-x1_lmin_value2)*(1-threshold_30)\n",
    "                SOS_intercect2 = x1_lmin_value2+abs(x1_lmax_value1-x1_lmin_value2)*threshold_30\n",
    "                SOS_EOS_intersect[0] = SOS_intercect2\n",
    "                SOS_EOS_intersect[1] = EOS_intercect1\n",
    "                SOS_EOS_intersect[2] = SOS_intercect1\n",
    "                SOS_EOS_intersect[3] = EOS_intercect2\n",
    "    \n",
    "                # find the root for SOS and EOS (should lie between min and max index)\n",
    "                tck_x = dataframe.index.values[x1_lmax_pos1:x1_lmin_pos1+1]+1\n",
    "                tck_y = x1[x1_lmax_pos1:x1_lmin_pos1+1]\n",
    "                tck1 = splrep(tck_x, tck_y-EOS_intercect1, s=0)\n",
    "                ppoly1 = PPoly.from_spline(tck1)\n",
    "                ppoly_r1 = ppoly1.roots(extrapolate=False)\n",
    "                if (len(ppoly_r1)>1):\n",
    "                    SOS_EOS_DOY[1] = ppoly_r1[-1]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[1] = ppoly_r1\n",
    "            \n",
    "                tck_x = dataframe.index.values[x1_lmin_pos1:x1_lmax_pos2+1]+1\n",
    "                tck_y = x1[x1_lmin_pos1:x1_lmax_pos2+1]\n",
    "                tck2 = splrep(tck_x, tck_y-SOS_intercect1, s=0)\n",
    "                ppoly2 = PPoly.from_spline(tck2)\n",
    "                ppoly_r2 = ppoly2.roots(extrapolate=False)\n",
    "                if (len(ppoly_r2)>1):\n",
    "                    SOS_EOS_DOY[2] = ppoly_r2[0]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[2] = ppoly_r2\n",
    "            \n",
    "                tck_x = dataframe.index.values[x1_lmax_pos2:x1_lmin_pos2+1]+1\n",
    "                tck_y = x1[x1_lmax_pos2:x1_lmin_pos2+1]\n",
    "                tck3 = splrep(tck_x, tck_y-EOS_intercect2, s=0)\n",
    "                ppoly3 = PPoly.from_spline(tck3)\n",
    "                ppoly_r3 = ppoly3.roots(extrapolate=False)\n",
    "                if (len(ppoly_r3)>1):\n",
    "                    SOS_EOS_DOY[3] = ppoly_r3[-1]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[3] = ppoly_r3\n",
    "            \n",
    "                tck_x = dataframe.index.values[x1_lmin_pos2:x1_lmax_pos1+365+1]+1\n",
    "                tck_y = x1[x1_lmin_pos2:x1_lmax_pos1+365+1]\n",
    "                tck4 = splrep(tck_x, tck_y-SOS_intercect2, s=0)\n",
    "                ppoly4 = PPoly.from_spline(tck4)\n",
    "                ppoly_r4 = ppoly4.roots(extrapolate=False)\n",
    "                if (len(ppoly_r4)>1):\n",
    "                    SOS_EOS_DOY[0] = ppoly_r4[0]\n",
    "                else:\n",
    "                    SOS_EOS_DOY[0] = ppoly_r4\n",
    "                # LOS[0] = (SOS_EOS_DOY[1]-(SOS_EOS_DOY[0]-365))+(SOS_EOS_DOY[3]-SOS_EOS_DOY[2])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # return SOS_EOS_DOY, SOS_EOS_intersect, x1_result, LOS\n",
    "        return SOS_EOS_DOY, SOS_EOS_intersect, x1_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle the exception\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da071ba0-8869-465a-9d78-b46c416a8006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df, var):\n",
    "    df_no_outlier = df.copy()\n",
    "    df_no_outlier = df_no_outlier.reset_index()\n",
    "    outlier_mask = np.full(len(df_no_outlier), False)\n",
    "\n",
    "    df_no_outlier['hour_start'] = df_no_outlier['TIMESTAMP_START'].dt.hour\n",
    "    \n",
    "    for idx, row in df_no_outlier.iterrows():\n",
    "        center_time = row['TIMESTAMP_END']\n",
    "        window_start = center_time - pd.Timedelta(days=7)\n",
    "        window_end = center_time + pd.Timedelta(days=7)\n",
    "\n",
    "        hour_diff = (df_no_outlier['hour_start'] - row['hour_start']) % 24\n",
    "        same_time_mask = (\n",
    "            (df_no_outlier['TIMESTAMP_END'] >= window_start) &\n",
    "            (df_no_outlier['TIMESTAMP_END'] <= window_end) &\n",
    "            (hour_diff <= 2) &\n",
    "            (~df_no_outlier[var].isna())\n",
    "        )\n",
    "\n",
    "        window_values = df_no_outlier.loc[same_time_mask, var]\n",
    "\n",
    "            \n",
    "        if len(window_values) < 5:  # Not enough data to judge\n",
    "            continue\n",
    "\n",
    "        if not window_values.empty:\n",
    "            mean_val = window_values.mean()\n",
    "            std_val = window_values.std()\n",
    "\n",
    "            # allow noiser fluxes if the temperature is low (set 5-day mean air temperature lower than 5 degree C as criteria)\n",
    "            # when 5-day mean air temperature is low, use mean+-5*std to filter the outlier\n",
    "            # remove precipitation data when the temperature is low (found this problem because wrong and extreme high annual precipitation in US-NGC and US-xWD)\n",
    "            temp_mask = ((df_no_outlier['TIMESTAMP_END'] >= row['TIMESTAMP_END'] - pd.Timedelta(days=2)) & \n",
    "                        (df_no_outlier['TIMESTAMP_END'] <= row['TIMESTAMP_END'] + pd.Timedelta(days=2)))\n",
    "            if 'TA_low' in df_no_outlier.columns:\n",
    "                TA_5day_mean = df_no_outlier.loc[temp_mask, 'TA_low'].mean()\n",
    "            else:\n",
    "                TA_5day_mean = 10 # Assume above freezing if no TA_freezing data\n",
    "\n",
    "            if (TA_5day_mean < 5) :\n",
    "                upper_bound = mean_val + 5*std_val\n",
    "                lower_bound = mean_val - 5*std_val\n",
    "                low_TA = 1 # True\n",
    "            else:\n",
    "                upper_bound = mean_val + 3*std_val\n",
    "                lower_bound = mean_val - 3*std_val\n",
    "                low_TA = 0 # False\n",
    "\n",
    "        if var in ['GPP_DT_VUT_REF', 'GPP']:\n",
    "            if ((row[var] > upper_bound) | (row[var] < lower_bound)):\n",
    "                outlier_mask[idx] = True\n",
    "        else:\n",
    "            if (low_TA == 1):\n",
    "                outlier_mask[idx] = True\n",
    "\n",
    "    df_no_outlier.loc[outlier_mask, var] = np.nan\n",
    "    # df_no_outlier.drop(columns=['hour_start'], inplace=True)\n",
    "    # .drop(columns=['hour_start', 'TIMESTAMP_START'], inplace=True)\n",
    "\n",
    "    return df_no_outlier\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "915c4bf7-c224-43a5-809a-0821047648a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot(args):\n",
    "\n",
    "    site_ID, site_name, site_year_start, site_year_end, site_IGBP, site_src = args\n",
    "    \n",
    "    print(f\"in process_and_plot {site_ID} and source from {site_src}\")\n",
    "    \n",
    "    if (site_src == 'ICOS'):\n",
    "        # site setting\n",
    "        zip_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/ICOS/warm_winter_2020_release_2022/'\n",
    "        pwd_dir = os.listdir('/burg/glab/users/rg3390/data/FLUXNET2015/ICOS/warm_winter_2020_release_2022/.')\n",
    "        pattern = 'FLX_'+site_ID+'_*_FULLSET_*.zip'\n",
    "        unzip_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/ICOS/warm_winter_2020_release_2022/tmp_unzip/'\n",
    "        pos_sitestr_start = 5-1\n",
    "        pos_sitestr_end = 10\n",
    "        pos_yearstart_start = 32-1\n",
    "        pos_yearstart_end = 35\n",
    "        pos_yearend_start = 37-1\n",
    "        pos_yearend_end = 40\n",
    "\n",
    "    elif (site_src == 'AmeriFlux'):\n",
    "        zip_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/AmeriFlux/'\n",
    "        pwd_dir = os.listdir('/burg/glab/users/rg3390/data/FLUXNET2015/AmeriFlux/.')\n",
    "        pattern = 'AMF_'+site_ID+'_*_FULLSET_*.zip'\n",
    "        unzip_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/AmeriFlux/tmp_unzip/'\n",
    "        pos_sitestr_start = 5-1\n",
    "        pos_sitestr_end = 10\n",
    "        pos_yearstart_start = 28-1\n",
    "        pos_yearstart_end = 31\n",
    "        pos_yearend_start = 33-1\n",
    "        pos_yearend_end = 36\n",
    "    \n",
    "    elif (site_src == 'FLUXNET'):\n",
    "        zip_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/FLUXNET2015_zipped/'\n",
    "        pwd_dir = os.listdir('/burg/glab/users/rg3390/data/FLUXNET2015/FLUXNET2015_zipped/.')\n",
    "        pattern = 'FLX_'+site_ID+'_*_FULLSET_*.zip'\n",
    "        unzip_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/FLUXNET2015_zipped/tmp_unzip/'\n",
    "        pos_sitestr_start = 5-1\n",
    "        pos_sitestr_end = 10\n",
    "        pos_yearstart_start = 32-1\n",
    "        pos_yearstart_end = 35\n",
    "        pos_yearend_start = 37-1\n",
    "        pos_yearend_end = 40\n",
    "\n",
    "    elif (site_src == 'Ozflux'): # Ozflux\n",
    "        nc_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/OzFlux/'\n",
    "        pwd_dir = os.listdir('/burg/glab/users/rg3390/data/FLUXNET2015/OzFlux/.')\n",
    "        pattern = site_name+'_L6.nc'\n",
    "\n",
    "    elif (site_src == 'LBA-ECO'):\n",
    "        xlsx_dir = '/burg/glab/users/rg3390/data/FLUXNET2015/LBA-ECO/'\n",
    "        pattern = site_ID+'_CfluxBF.xlsx'\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(f\"choosing directory for {site_ID} and source from {site_src}\")\n",
    "\n",
    "    # processing flux data\n",
    "    if ((site_src == 'ICOS') | (site_src == 'AmeriFlux') | (site_src == 'FLUXNET')):\n",
    "        for entry in pwd_dir:\n",
    "            out_columns = []\n",
    "            if fnmatch.fnmatch(entry, pattern):\n",
    "                a1 = entry[pos_sitestr_start:pos_sitestr_end]\n",
    "                a2 = str(int(entry[pos_yearstart_start:pos_yearstart_end]))\n",
    "                a3 = str(int(entry[pos_yearend_start:pos_yearend_end]))\n",
    "\n",
    "                # unzip the zip files\n",
    "                with zipfile.ZipFile(zip_dir+entry,\"r\") as zip_ref:\n",
    "                    zip_ref.extractall(unzip_dir)\n",
    "\n",
    "                # Import hourly or half-hourly fluxes data\n",
    "                len_HH = len(glob.glob(unzip_dir + f\"*{site_ID}*FULLSET_HH*.csv\"))\n",
    "                print(glob.glob(unzip_dir + f\"*{site_ID}*FULLSET_HH*.csv\"))\n",
    "                if  (len_HH == 1):\n",
    "                    flux_df = pd.read_csv(glob.glob(unzip_dir + f\"*{site_ID}*FULLSET_HH*.csv\")[0], usecols=lambda x: x in pick_cols) # Use half hourly data\n",
    "                    day_timestep = 48 # 48 timestep in a day\n",
    "                    rain_QC_timestep = 24 # rain within 12 hours\n",
    "                else:\n",
    "                    flux_df = pd.read_csv(glob.glob(unzip_dir + f\"*{site_ID}*FULLSET_HR*.csv\")[0], usecols=lambda x: x in pick_cols) # Use hourly data\n",
    "                    day_timestep = 24 # 24 timestep in a day\n",
    "                    rain_QC_timestep = 12 # rain within 12 hours\n",
    "                    \n",
    "                flux_df[\"TIMESTAMP_START\"] = pd.to_datetime(flux_df[\"TIMESTAMP_START\"], format=\"%Y%m%d%H%M\")\n",
    "                flux_df[\"TIMESTAMP_END\"] = pd.to_datetime(flux_df[\"TIMESTAMP_END\"], format=\"%Y%m%d%H%M\")\n",
    "                flux_df[\"time_year\"] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "                print(\"site = \", a1, \", HH = \", len_HH) # HH=1 is half-hourly data\n",
    "\n",
    "                # ## convert unit\n",
    "                # flux_df[\"PA_F\"] = flux_df[\"PA_F\"]*10 # kPa -> hPa\n",
    "                # # VPD unit already hPa\n",
    "\n",
    "                # create a dataframe to save good data\n",
    "                site_data = pd.DataFrame({'TIMESTAMP_START': flux_df['TIMESTAMP_START'], 'TIMESTAMP_END': flux_df['TIMESTAMP_END']})\n",
    "\n",
    "                ## Quality control 1\n",
    "                flux_df = flux_df.replace(-9999,np.nan)\n",
    "                for it, (var, qc) in enumerate(dic_QC.items()):\n",
    "                    if qc in flux_df.columns:\n",
    "                        # flux_df = flux_df[flux_df[qc].isin([0, 1])]\n",
    "                        flux_df.loc[(flux_df[qc] >= 2), var] = np.nan\n",
    "                print(f\"complete {a1} QC 1 (from QC flags)\")\n",
    "\n",
    "                # remove rain in previous 12 hours\n",
    "                flux_df.loc[len(flux_df)] = {col: np.nan for col in flux_df.columns} # add a nan row because df[\"var\"][a:b] does not count in df[\"var\"][b]\n",
    "                for pt in range(rain_QC_timestep*1, flux_df.shape[0]-1):\n",
    "                    if ((np.nansum(flux_df.loc[pt-rain_QC_timestep:pt+1, \"P_F\"])==0) | (np.isnan(np.nansum(flux_df.loc[pt-rain_QC_timestep:pt+1, \"P_F\"])))):\n",
    "                        pass\n",
    "                    else:\n",
    "                        flux_df.loc[pt, \"USTAR\"] = np.nan\n",
    "                        flux_df.loc[pt, \"GPP_DT_VUT_REF\"] = np.nan\n",
    "\n",
    "                ## Quality control 2\n",
    "                # keep the flux data when u* >= 0.2 and < upper bound (mean+2*std)\n",
    "                # Get u* upper bound -> for Quality control 2\n",
    "                USTAR_mean = flux_df[\"USTAR\"].mean(axis=0)\n",
    "                USTAR_std = flux_df[\"USTAR\"].std(axis=0)\n",
    "                USTAR_upper = USTAR_mean+2*USTAR_std\n",
    "                flux_df.loc[((flux_df[\"USTAR\"]<=0.2) | (flux_df[\"USTAR\"]>USTAR_upper)), 'GPP_DT_VUT_REF'] = np.nan\n",
    "                print(f\"complete {a1} QC 2 (exclude too low and too high USTAR for fluxes)\")\n",
    "\n",
    "                ## Quality Control 3 - remove outliers\n",
    "                site_data['time_year'] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "                site_data['SW_IN_F'] = flux_df[\"SW_IN_F\"]\n",
    "                for it, (var, qc) in enumerate(dic_QC_flux.items()):\n",
    "                    # pick variable column and keep good or no qc flags data\n",
    "                    df_var = flux_df[['TIMESTAMP_START', 'TIMESTAMP_END', 'SW_IN_F', var]].copy()\n",
    "                    # copy Tair data for filtering freezing conditions\n",
    "                    df_var['TA_low'] = flux_df['TA_F']\n",
    "                    df_var['TA_low_QC'] = flux_df['TA_F_QC']\n",
    "                    if df_var['TA_low'].notna().any():\n",
    "                        df_var['TA_low'] = df_var['TA_low'].where(df_var['TA_low_QC'].isin([0,1]), np.nan)\n",
    "                    else:\n",
    "                        pass\n",
    "                    df_var.drop('TA_low_QC', axis=1, inplace=True)\n",
    "    \n",
    "                    if var in ['GPP_DT_VUT_REF']:\n",
    "                        print(f\"start outlier removing {site_ID} - {var}\")\n",
    "                        df_var_no_outlier = remove_outlier(df_var, var)\n",
    "                        df_var_no_outlier.drop(columns=['TA_low', 'hour_start'], inplace=True)\n",
    "                        print(f\"finish outlier removing {site_ID} - {var}\")\n",
    "                    else:\n",
    "                        df_var_no_outlier = df_var.copy()\n",
    "                        df_var_no_outlier.drop(columns=['TA_low'], inplace=True)\n",
    "    \n",
    "                    # merge good data into dataframe\n",
    "                    site_data = site_data.merge(df_var_no_outlier[['TIMESTAMP_START', 'TIMESTAMP_END', var]], on=['TIMESTAMP_START', 'TIMESTAMP_END'], how='left')\n",
    "                    del df_var, df_var_no_outlier\n",
    "                print(f\"complete {a1} QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "                ## select time period: daytime (SW>10 W/m2)\n",
    "                flux_time_mask = ((site_data[\"time_year\"] >= site_year_start) & \n",
    "                        (site_data[\"time_year\"] <= site_year_end) & \n",
    "                        (site_data[\"SW_IN_F\"]>=50))\n",
    "                site_data = site_data[flux_time_mask]\n",
    "\n",
    "                print(f\"complete {a1} selecting site_data\")\n",
    "\n",
    "                ## daily mean\n",
    "                site_data = site_data.set_index('TIMESTAMP_START')\n",
    "                site_data_daily_tmp = site_data['GPP_DT_VUT_REF'].resample('D').mean()\n",
    "                site_data_daily_tmp = site_data_daily_tmp.reset_index()\n",
    "                site_data_daily_tmp = site_data_daily_tmp.rename(columns={\"TIMESTAMP_START\": \"date\"})\n",
    "                site_data_daily = pd.DataFrame({'date': pd.date_range(start=f\"{int(site_year_start)}-01-01\", end=f\"{int(site_year_end)}-12-31\", freq='D')})\n",
    "                site_data_daily = pd.merge(site_data_daily, site_data_daily_tmp, on='date', how='left')\n",
    "                \n",
    "\n",
    "                ## interpolation\n",
    "                site_data_daily[\"GPP_DT_VUT_REF\"] = site_data_daily[\"GPP_DT_VUT_REF\"].interpolate(method='linear')\n",
    "                site_data_daily[\"GPP_DT_VUT_REF\"] = site_data_daily[\"GPP_DT_VUT_REF\"].rolling(window_size, min_periods=5, center=True).apply(lambda x : np.nanmedian(x))\n",
    "                site_data_daily.loc[np.isnan(site_data_daily[\"GPP_DT_VUT_REF\"]), \"GPP_DT_VUT_REF\"] = 0\n",
    "                print(f\"complete {a1} interpolation and rolling mean\")\n",
    "\n",
    "                ### --------------- get SOS, EOS every year ---------------\n",
    "                site_data_daily[\"DOY\"] = site_data_daily[\"date\"].dt.dayofyear\n",
    "                ## exclude all Feb 29 rows\n",
    "                site_data_daily = site_data_daily[~((site_data_daily['date'].dt.month == 2) & (site_data_daily['date'].dt.day == 29))]\n",
    "            \n",
    "                ## select year\n",
    "                site_data_daily['Year'] = site_data_daily['date'].dt.year\n",
    "            \n",
    "                ## create dataframe for SOS and EOS for each site\n",
    "                df_SOS_EOS = pd.DataFrame()\n",
    "                \n",
    "                ## smooth and scale data for each year\n",
    "                site_years = site_data_daily.loc[len(site_data_daily)-1, 'Year'] - site_data_daily.loc[0, 'Year'] + 1\n",
    "                site_year_start = int(site_data_daily.loc[0, 'Year'])\n",
    "                site_year_end = int(site_data_daily.loc[len(site_data_daily)-1, 'Year'])\n",
    "                count = 0\n",
    "                fig, axs = plt.subplots(site_years, 1, figsize=(8,3*site_years))\n",
    "                # for year, group in grouped_year:\n",
    "                for yt in range(site_year_start, site_year_end+1, 1):\n",
    "                    df_year_tmp = site_data_daily.copy()\n",
    "                    df_year = df_year_tmp[df_year_tmp['Year'] == yt]\n",
    "                    \n",
    "                    ## smoothing grouped_year (use the middle concat result to avoid unsmoothed tail)\n",
    "                    grouped_df_concat3 = pd.concat([df_year, df_year, df_year], axis=0)\n",
    "                    grouped_df_concat3 = grouped_df_concat3.reset_index()\n",
    "            \n",
    "                    smoothed_df_concat3 = grouped_df_concat3.copy()\n",
    "                    for it in range(50):\n",
    "                        smoothed_df_concat3 = smooth_savgol(smoothed_df_concat3, \"GPP_DT_VUT_REF\")\n",
    "                    print(f\"complete {site_ID} smoothing year = {yt}\")\n",
    "                    \n",
    "                    # ## plot: smoothed vs original\n",
    "                    # axs[count].plot(grouped_df_concat3.index.values, grouped_df_concat3[\"GPP_DT_VUT_REF\"])\n",
    "                    # axs[count].plot(smoothed_df_concat3.index.values, smoothed_df_concat3[\"GPP_DT_VUT_REF\"])\n",
    "                    # axs[count].set_title(f\"{year}\")\n",
    "                    # axs[count].set_xlim((365,365*2-1))\n",
    "\n",
    "                    ## scale to [0,1]\n",
    "                    smoothed_df = smoothed_df_concat3.copy()\n",
    "                    columns_to_normalize = \"GPP_DT_VUT_REF\"\n",
    "                    scaled_df = smoothed_df.copy()\n",
    "                    scaler = MinMaxScaler()\n",
    "                    scaled_df[columns_to_normalize] = scaler.fit_transform(smoothed_df[[columns_to_normalize]])\n",
    "                    print(f\"complete {site_ID} scaling year = {yt}\")\n",
    "                    print(scaled_df)\n",
    "\n",
    "                    ## get SOS, EOS\n",
    "                    SOS_EOS_DOY_GPP, SOS_EOS_intersect_GPP, x_result_GPP = find_SOS_EOS(scaled_df, \"GPP_DT_VUT_REF\")\n",
    "                    print(f\"get {site_ID} SOS, EOS in year = {yt}\")\n",
    "\n",
    "                    ## plot SOS, EOS\n",
    "                    axs[count].plot(grouped_df_concat3.index.values, scaled_df[\"GPP_DT_VUT_REF\"])\n",
    "                    print(f\"pass first line of plot {site_ID} in year = {yt}\")\n",
    "                    for it in range(len(x_result_GPP)):\n",
    "                        axs[count].axvline(x=x_result_GPP[it], color='black', linestyle='--')\n",
    "                    for it in range(len(SOS_EOS_DOY_GPP)):\n",
    "                        if (~np.isnan(SOS_EOS_DOY_GPP[it])):\n",
    "                            axs[count].plot(SOS_EOS_DOY_GPP[it], SOS_EOS_intersect_GPP[it], 'o')\n",
    "                        else:\n",
    "                            pass\n",
    "                    print(f\"pass second line of plot {site_ID} in year = {yt}\")\n",
    "                    axs[count].set_title(f\"{yt}\")\n",
    "                    axs[count].set_xlim((365,365*2-1))\n",
    "                    axs[count].set_xticks(np.arange(365,365*2,60), np.arange(0,365,60)+1)\n",
    "                    axs[count].set_ylabel('GPP')\n",
    "                    \n",
    "                    ## fill in SOS, EOS\n",
    "                    df_SOS_EOS.loc[count, \"site_ID\"] = site_ID\n",
    "                    df_SOS_EOS.loc[count, \"IGBP\"] = site_IGBP\n",
    "                    df_SOS_EOS.loc[count, \"year\"] = int(yt)\n",
    "                    df_SOS_EOS.loc[count, \"SOS_1_GPP\"] = SOS_EOS_DOY_GPP[0]-365\n",
    "                    df_SOS_EOS.loc[count, \"EOS_1_GPP\"] = SOS_EOS_DOY_GPP[1]-365\n",
    "                    df_SOS_EOS.loc[count, \"SOS_2_GPP\"] = SOS_EOS_DOY_GPP[2]-365\n",
    "                    df_SOS_EOS.loc[count, \"EOS_2_GPP\"] = SOS_EOS_DOY_GPP[3]-365\n",
    "                    \n",
    "                    count = count + 1\n",
    "\n",
    "                    del grouped_df_concat3, smoothed_df_concat3, smoothed_df, scaled_df\n",
    "                \n",
    "                del site_data_daily, flux_df\n",
    "                # print(f\"get {a1} SOS and EOS\")\n",
    "                \n",
    "\n",
    "                ## save figure\n",
    "                filename = f\"scaled_SOS_EOS_{site_ID}.png\"\n",
    "                # filename = f\"smooth_SOS_EOS_{site_ID}.png\"\n",
    "                fig.savefig(dir_savefig+filename, dpi=400, bbox_inches='tight')\n",
    "                print(f\"Saved png: {filename}\")\n",
    "                # plt.close()\n",
    "            \n",
    "                # --- Save filtered site data ---\n",
    "                fn_csv_out = f\"{site_ID}_SOS_EOS.csv\"\n",
    "                df_SOS_EOS.fillna(-9999).to_csv(dir_csv_out + fn_csv_out, index=False)\n",
    "                print(f\"Saved csv: {fn_csv_out}\")\n",
    "\n",
    "                if len_HH == 1:\n",
    "                    pattern = os.path.join(unzip_dir, f\"*{site_ID}*.csv\")\n",
    "                else:\n",
    "                    pattern = os.path.join(unzip_dir, f\"*{site_ID}*.csv\")\n",
    "                \n",
    "                for file_path in glob.glob(pattern):\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Deleted: {file_path}\")\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "        # ### delete the unzip folder\n",
    "        # shutil.rmtree(unzip_dir)\n",
    "        \n",
    "            \n",
    "\n",
    "    elif (site_src == 'Ozflux'):\n",
    "        try:\n",
    "            ## create a dataframe in filtering negative sensible heat flux\n",
    "            flux_df = pd.DataFrame(columns=pick_cols_Ozflux)\n",
    "    \n",
    "            ## read site data from netcdf files\n",
    "            flux_ds = xr.open_dataset(nc_dir+pattern)\n",
    "            print(f\"-- read OzFlux data from {nc_dir+pattern}\")\n",
    "            flux_df[\"TIMESTAMP_END\"] = flux_ds[\"time\"]\n",
    "            flux_df[\"P_F\"] = np.squeeze(flux_ds[\"Precip\"])\n",
    "            flux_df[\"P_F_QC\"] = np.squeeze(flux_ds[\"Precip_QCFlag\"])\n",
    "            # flux_df[\"LE_flux\"] = np.squeeze(flux_ds[\"Fe\"])\n",
    "            # flux_df[\"LE_flux_QC\"] = np.squeeze(flux_ds[\"Fe_QCFlag\"])\n",
    "            # flux_df[\"H_flux\"] = np.squeeze(flux_ds[\"Fh\"])\n",
    "            # flux_df[\"H_flux_QC\"] = np.squeeze(flux_ds[\"Fh_QCFlag\"])\n",
    "            flux_df[\"TA_F\"] = np.squeeze(flux_ds[\"Ta\"]) # degree C\n",
    "            flux_df[\"TA_F_QC\"] = np.squeeze(flux_ds[\"Ta_QCFlag\"])\n",
    "            # flux_df[\"PA_F\"] = np.squeeze(flux_ds[\"ps\"]) # kPa\n",
    "            # flux_df[\"PA_F_QC\"] = np.squeeze(flux_ds[\"ps_QCFlag\"])\n",
    "            if \"GPP_LL\" in flux_ds:\n",
    "                ## select GPP product\n",
    "                flux_df[\"GPP\"] = np.squeeze(flux_ds[\"GPP_LL\"])\n",
    "                flux_df[\"GPP_QC\"] = np.squeeze(flux_ds[\"GPP_LL_QCFlag\"]) # same as GPP_DT_VUT_REF\n",
    "            else:\n",
    "                flux_df[\"GPP\"] = np.squeeze(flux_ds[\"GPP_SOLO\"])\n",
    "                flux_df[\"GPP_QC\"] = np.squeeze(flux_ds[\"GPP_SOLO_QCFlag\"])\n",
    "            # flux_df[\"VPD_F\"] = np.squeeze(flux_ds[\"VPD\"]) # kPa\n",
    "            # flux_df[\"VPD_F_QC\"] = np.squeeze(flux_ds[\"VPD_QCFlag\"])\n",
    "            flux_df[\"SW_IN_F\"] = np.squeeze(flux_ds[\"Fsd\"])\n",
    "            flux_df[\"SW_IN_F_QC\"] = np.squeeze(flux_ds[\"Fsd_QCFlag\"])\n",
    "            # flux_df[\"WS_F\"] = np.squeeze(flux_ds[\"Ws\"])\n",
    "            # flux_df[\"WS_F_QC\"] = np.squeeze(flux_ds[\"Ws_QCFlag\"])\n",
    "            # flux_df[\"LW_IN_F\"] = np.squeeze(flux_ds[\"Fld\"])\n",
    "            # flux_df[\"LW_IN_F_QC\"] = np.squeeze(flux_ds[\"Fld_QCFlag\"])\n",
    "            # flux_df[\"NETRAD\"] = np.squeeze(flux_ds[\"Fn\"])\n",
    "            # flux_df[\"NETRAD_QC\"] = np.squeeze(flux_ds[\"Fn_QCFlag\"])\n",
    "            # flux_df[\"G_flux\"] = np.squeeze(flux_ds[\"Fg\"])\n",
    "            # flux_df[\"G_flux_QC\"] = np.squeeze(flux_ds[\"Fg_QCFlag\"])\n",
    "            flux_df[\"USTAR\"] = np.squeeze(flux_ds[\"ustar\"])\n",
    "            flux_df[\"USTAR_QC\"] = np.squeeze(flux_ds[\"ustar_QCFlag\"])\n",
    "            flux_ds.close()\n",
    "            del flux_ds\n",
    "    \n",
    "            # ## convert unit\n",
    "            # flux_df[\"PA_F\"] = flux_df[\"PA_F\"]*10 # kPa -> hPa\n",
    "            # flux_df[\"VPD_F\"] = flux_df[\"VPD_F\"]*10 # kPa -> hPa\n",
    "    \n",
    "            ## check whether data is half-hourly or hourly\n",
    "            if (flux_df[\"TIMESTAMP_END\"].iloc[1]-flux_df[\"TIMESTAMP_END\"].iloc[0] == pd.Timedelta(minutes=30)):\n",
    "                len_HH = 1\n",
    "                flux_df[\"TIMESTAMP_START\"] = flux_df[\"TIMESTAMP_END\"] - pd.Timedelta(minutes=30)\n",
    "            else:\n",
    "                len_HH = 0\n",
    "                flux_df[\"TIMESTAMP_START\"] = flux_df[\"TIMESTAMP_END\"] - pd.Timedelta(minutes=60)\n",
    "            print(f\"site = {site_ID} , HH = {len_HH}\") # HH=1 is half-hourly data\n",
    "            if  (len_HH == 1):\n",
    "                day_timestep = 48 # 48 timestep in a day\n",
    "                rain_QC_timestep = 24 # rain within 12 hours\n",
    "            else:\n",
    "                day_timestep = 24 # 24 timestep in a day\n",
    "                rain_QC_timestep = 12 # rain within 12 hours\n",
    "    \n",
    "            ## Quality control 1\n",
    "            flux_df = flux_df.replace(-9999,np.nan)\n",
    "            for var in var_to_QC_Ozflux:\n",
    "                # Data that has been corrected is recommended for process-based studies.\n",
    "                # Data that has been gap filled is not recommended for process-based studies\n",
    "                # but may be used for budget-style studies.\n",
    "                # source: https://github.com/OzFlux/PyFluxPro/wiki/QC-Flag-Definitions\n",
    "                # where the condition is true, keep the original values, otherwise, default replace with nan\n",
    "                flux_df[var] = flux_df[var].where(((np.all(np.isnan(flux_df[dic_QC_Ozflux[var]]))) |\n",
    "                                                   (flux_df[dic_QC_Ozflux[var]]==0) |\n",
    "                                                   (flux_df[dic_QC_Ozflux[var]]==10) |\n",
    "                                                   (flux_df[dic_QC_Ozflux[var]]==20)))\n",
    "            print(f\"complete {site_ID} QC 1 (from QC flags)\")\n",
    "            \n",
    "            # # 0<=RH<=100\n",
    "            # flux_df[\"e_sat\"] = es0*np.exp((Lv/Rv)*((1/T0)-(1/(flux_df[\"TA_F\"]+273.15))))\n",
    "            # flux_df[\"RH\"] = 100*(flux_df[\"e_sat\"]-flux_df[\"VPD_F\"])/flux_df[\"e_sat\"]\n",
    "            # flux_df[\"RH\"] = flux_df[\"RH\"].where((flux_df[\"RH\"]>=0) & (flux_df[\"RH\"]<=100))\n",
    "    \n",
    "            # remove rainy event (rain within previous 12 hours) from flux data\n",
    "            flux_df.loc[len(flux_df)] = {col: np.nan for col in flux_df.columns} # add a nan row because df[\"var\"][a:b] does not count in df[\"var\"][b]\n",
    "            for pt in range(rain_QC_timestep*1, flux_df.shape[0]):\n",
    "                if ((np.all(~np.isnan(flux_df[\"P_F\"].iloc[pt-rain_QC_timestep:pt+1]))) & (np.sum(flux_df[\"P_F\"].iloc[pt-rain_QC_timestep:pt+1])==0)):\n",
    "                    # if only np.sum, still count in nan\n",
    "                    pass\n",
    "                else:\n",
    "                    flux_df[\"USTAR\"].iloc[pt] = np.nan\n",
    "                    # flux_df[\"H_flux\"].iloc[pt] = np.nan\n",
    "                    # flux_df[\"LE_flux\"].iloc[pt] = np.nan\n",
    "                    flux_df[\"GPP\"].iloc[pt] = np.nan\n",
    "                    # flux_df[\"G_flux\"].iloc[pt] = np.nan\n",
    "\n",
    "            ## Get u* upper bound -> for Quality control 2\n",
    "            USTAR_mean = flux_df[\"USTAR\"].mean(axis=0)\n",
    "            USTAR_std = flux_df[\"USTAR\"].std(axis=0)\n",
    "            USTAR_upper = USTAR_mean+2*USTAR_std\n",
    "            flux_df.loc[((flux_df[\"USTAR\"]<=0.2) | (flux_df[\"USTAR\"]>USTAR_upper)), 'GPP'] = np.nan\n",
    "            print(f\"complete {site_ID} QC 2 (exclude too low and too high USTAR for fluxes)\")\n",
    "            # print(flux_df.loc[5760:5790, \"GPP_DT_VUT_REF\"])\n",
    "            # print(f\"{site_ID} {flux_df}\")\n",
    "    \n",
    "            # ## Quality Control 3 - remove outliers\n",
    "            # site_data['time_year'] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "            # site_data['SW_IN_F'] = flux_df[\"SW_IN_F\"]\n",
    "            # for it, (var, qc) in enumerate(dic_QC_Ozflux_flux.items()):\n",
    "            #     # pick variable column and keep good or no qc flags data\n",
    "            #     df_var = flux_df[['TIMESTAMP_START', 'TIMESTAMP_END', 'SW_IN_F', var]].copy()\n",
    "            #     print(f\"{site_ID} {df_var}\")\n",
    "            #     # copy Tair data for filtering freezing conditions\n",
    "            #     df_var['TA_low'] = flux_df['TA_F']\n",
    "            #     df_var['TA_low_QC'] = flux_df['TA_F_QC']\n",
    "            #     if df_var['TA_low'].notna().any():\n",
    "            #         df_var['TA_low'] = df_var['TA_low'].where(df_var['TA_low_QC'].isin([0,10,20]), np.nan)\n",
    "            #     else:\n",
    "            #         pass\n",
    "            #     df_var.drop('TA_low_QC', axis=1, inplace=True)\n",
    "    \n",
    "            #     if var in ['GPP']:\n",
    "            #         print(f\"start outlier removing {site_ID} - {var}\")\n",
    "            #         df_var_no_outlier = remove_outlier(df_var, var)\n",
    "            #         df_var_no_outlier.drop(columns=['TA_low', 'hour_start'], inplace=True)\n",
    "            #         print(f\"finish outlier removing {site_ID} - {var}\")\n",
    "            #     else:\n",
    "            #         df_var_no_outlier = df_var.copy()\n",
    "            #         df_var_no_outlier.drop(columns=['TA_low'], inplace=True)\n",
    "    \n",
    "            #     # merge good data into dataframe\n",
    "            #     site_data = site_data.merge(df_var_no_outlier[['TIMESTAMP_START', 'TIMESTAMP_END', var]], on=['TIMESTAMP_START', 'TIMESTAMP_END'], how='left')\n",
    "            #     del df_var, df_var_no_outlier\n",
    "            # print(f\"complete {site_ID} QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "            # --- Quality Control 3 - remove outliers (robust) ---\n",
    "            required_base = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"SW_IN_F\"]\n",
    "            missing_base = [c for c in required_base if c not in flux_df.columns]\n",
    "            if missing_base:\n",
    "                raise KeyError(f\"[{site_ID}] Missing base columns before QC3: {missing_base}\")\n",
    "            \n",
    "            # ensure site_data exists and aligns\n",
    "            site_data = pd.DataFrame({\n",
    "                \"TIMESTAMP_START\": flux_df[\"TIMESTAMP_START\"],\n",
    "                \"TIMESTAMP_END\": flux_df[\"TIMESTAMP_END\"],\n",
    "            })\n",
    "            site_data[\"time_year\"] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "            site_data[\"SW_IN_F\"]   = flux_df[\"SW_IN_F\"]\n",
    "            \n",
    "            # bring the dict into scope robustly (works even in child processes)\n",
    "            try:\n",
    "                _qc_map = dic_QC_Ozflux_flux\n",
    "            except NameError:\n",
    "                # fallback: minimal map for GPP\n",
    "                _qc_map = {\"GPP\": \"GPP_QC\"}\n",
    "                print(f\"[{site_ID}] dic_QC_Ozflux_flux not found in worker; using fallback: {_qc_map}\")\n",
    "            \n",
    "            if not isinstance(_qc_map, dict) or not _qc_map:\n",
    "                print(f\"[{site_ID}] QC map empty or not a dict: {_qc_map} — skipping QC3 loop.\")\n",
    "            else:\n",
    "                # keep only variables that exist\n",
    "                vars_present = [v for v in _qc_map.keys() if v in flux_df.columns]\n",
    "                if not vars_present:\n",
    "                    print(f\"[{site_ID}] None of {_qc_map.keys()} present in flux_df; skipping QC3 loop.\")\n",
    "                else:\n",
    "                    print(f\"[{site_ID}] QC3 will process variables: {vars_present}\")\n",
    "            \n",
    "                for it, var in enumerate(vars_present):\n",
    "                    qc_col = _qc_map[var]\n",
    "                    need_cols = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"SW_IN_F\", var]\n",
    "                    missing_now = [c for c in need_cols if c not in flux_df.columns]\n",
    "                    if missing_now:\n",
    "                        print(f\"[{site_ID}] Skipping {var}: missing columns {missing_now}\")\n",
    "                        continue\n",
    "                    if qc_col not in flux_df.columns:\n",
    "                        print(f\"[{site_ID}] Skipping {var}: QC column '{qc_col}' not found.\")\n",
    "                        continue\n",
    "            \n",
    "                    df_var = flux_df[need_cols].copy()\n",
    "                    # Optional Tair screen if present\n",
    "                    if (\"TA_F\" in flux_df.columns) and (\"TA_F_QC\" in flux_df.columns):\n",
    "                        df_var[\"TA_low\"] = flux_df[\"TA_F\"].where(flux_df[\"TA_F_QC\"].isin([0, 10, 20]), np.nan)\n",
    "            \n",
    "                    if var in [\"GPP\"]:\n",
    "                        print(f\"[{site_ID}] start outlier removing - {var} (rows={len(df_var)})\")\n",
    "                        df_var_no_outlier = remove_outlier(df_var, var)\n",
    "                        df_var_no_outlier.drop(columns=[\"TA_low\", \"hour_start\"], inplace=True, errors=\"ignore\")\n",
    "                        print(f\"[{site_ID}] finish outlier removing - {var}\")\n",
    "                    else:\n",
    "                        df_var_no_outlier = df_var.copy()\n",
    "                        df_var_no_outlier.drop(columns=[\"TA_low\"], inplace=True, errors=\"ignore\")\n",
    "            \n",
    "                    # Merge back\n",
    "                    site_data = site_data.merge(\n",
    "                        df_var_no_outlier[[\"TIMESTAMP_START\", \"TIMESTAMP_END\", var]],\n",
    "                        on=[\"TIMESTAMP_START\", \"TIMESTAMP_END\"], how=\"left\"\n",
    "                    )\n",
    "                    del df_var, df_var_no_outlier\n",
    "            \n",
    "                print(f\"[{site_ID}] complete QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "            ## select time period: daytime (SW>10 W/m2)\n",
    "            flux_time_mask = ((site_data[\"time_year\"] >= site_year_start) & \n",
    "                    (site_data[\"time_year\"] <= site_year_end) & \n",
    "                    (site_data[\"SW_IN_F\"]>=50))\n",
    "            site_data = site_data[flux_time_mask]\n",
    "    \n",
    "            print(f\"complete {site_ID} selecting site_data\")\n",
    "    \n",
    "            ## daily mean\n",
    "            site_data = site_data.set_index('TIMESTAMP_START')\n",
    "            site_data_daily_tmp = site_data['GPP'].resample('D').mean()\n",
    "            site_data_daily_tmp = site_data_daily_tmp.reset_index()\n",
    "            site_data_daily_tmp = site_data_daily_tmp.rename(columns={\"TIMESTAMP_START\": \"date\"})\n",
    "            site_data_daily = pd.DataFrame({'date': pd.date_range(start=f\"{int(site_year_start)}-01-01\", end=f\"{int(site_year_end)}-12-31\", freq='D')})\n",
    "            site_data_daily = pd.merge(site_data_daily, site_data_daily_tmp, on='date', how='left')\n",
    "            \n",
    "    \n",
    "            ## interpolation\n",
    "            site_data_daily[\"GPP\"] = site_data_daily[\"GPP\"].interpolate(method='linear')\n",
    "            site_data_daily[\"GPP\"] = site_data_daily[\"GPP\"].rolling(window_size, min_periods=5, center=True).apply(lambda x : np.nanmedian(x))\n",
    "            site_data_daily.loc[np.isnan(site_data_daily[\"GPP\"]), \"GPP\"] = 0\n",
    "            print(f\"complete {site_ID} interpolation and rolling mean\")\n",
    "    \n",
    "            ### --------------- get SOS, EOS every year ---------------\n",
    "            site_data_daily[\"DOY\"] = site_data_daily[\"date\"].dt.dayofyear\n",
    "            ## exclude all Feb 29 rows\n",
    "            site_data_daily = site_data_daily[~((site_data_daily['date'].dt.month == 2) & (site_data_daily['date'].dt.day == 29))]\n",
    "        \n",
    "            ## select year\n",
    "            site_data_daily['Year'] = site_data_daily['date'].dt.year\n",
    "        \n",
    "            ## create dataframe for SOS and EOS for each site\n",
    "            df_SOS_EOS = pd.DataFrame()\n",
    "\n",
    "            ## smooth and scale data for each year\n",
    "            site_years = site_data_daily.loc[len(site_data_daily)-1, 'Year'] - site_data_daily.loc[0, 'Year'] + 1\n",
    "            site_year_start = int(site_data_daily.loc[0, 'Year'])\n",
    "            site_year_end = int(site_data_daily.loc[len(site_data_daily)-1, 'Year'])\n",
    "            count = 0\n",
    "            fig, axs = plt.subplots(site_years, 1, figsize=(8,3*site_years))\n",
    "            # for year, group in grouped_year:\n",
    "            for yt in range(site_year_start, site_year_end+1, 1):\n",
    "                df_year_tmp = site_data_daily.copy()\n",
    "                df_year = df_year_tmp[df_year_tmp['Year'] == yt]\n",
    "                \n",
    "                ## smoothing grouped_year (use the middle concat result to avoid unsmoothed tail)\n",
    "                grouped_df_concat3 = pd.concat([df_year, df_year, df_year], axis=0)\n",
    "                grouped_df_concat3 = grouped_df_concat3.reset_index()\n",
    "        \n",
    "                smoothed_df_concat3 = grouped_df_concat3.copy()\n",
    "                for it in range(50):\n",
    "                    smoothed_df_concat3 = smooth_savgol(smoothed_df_concat3, \"GPP\")\n",
    "                print(f\"complete {site_ID} smoothing year = {yt}\")\n",
    "                \n",
    "                # ## plot: smoothed vs original\n",
    "                # axs[count].plot(grouped_df_concat3.index.values, grouped_df_concat3[\"GPP_DT_VUT_REF\"])\n",
    "                # axs[count].plot(smoothed_df_concat3.index.values, smoothed_df_concat3[\"GPP_DT_VUT_REF\"])\n",
    "                # axs[count].set_title(f\"{year}\")\n",
    "                # axs[count].set_xlim((365,365*2-1))\n",
    "    \n",
    "                ## scale to [0,1]\n",
    "                smoothed_df = smoothed_df_concat3.copy()\n",
    "                columns_to_normalize = \"GPP\"\n",
    "                scaled_df = smoothed_df.copy()\n",
    "                scaler = MinMaxScaler()\n",
    "                scaled_df[columns_to_normalize] = scaler.fit_transform(smoothed_df[[columns_to_normalize]])\n",
    "                print(f\"complete {site_ID} scaling year = {yt}\")\n",
    "                # print(scaled_df)\n",
    "    \n",
    "                ## get SOS, EOS\n",
    "                SOS_EOS_DOY_GPP, SOS_EOS_intersect_GPP, x_result_GPP = find_SOS_EOS(scaled_df, \"GPP\")\n",
    "                print(f\"get {site_ID} SOS, EOS in year = {yt}\")\n",
    "    \n",
    "                ## plot SOS, EOS\n",
    "                axs[count].plot(grouped_df_concat3.index.values, scaled_df[\"GPP\"])\n",
    "                print(f\"pass first line of plot {site_ID} in year = {yt}\")\n",
    "                for it in range(len(x_result_GPP)):\n",
    "                    axs[count].axvline(x=x_result_GPP[it], color='black', linestyle='--')\n",
    "                for it in range(len(SOS_EOS_DOY_GPP)):\n",
    "                    if (~np.isnan(SOS_EOS_DOY_GPP[it])):\n",
    "                        axs[count].plot(SOS_EOS_DOY_GPP[it], SOS_EOS_intersect_GPP[it], 'o')\n",
    "                    else:\n",
    "                        pass\n",
    "                print(f\"pass second line of plot {site_ID} in year = {yt}\")\n",
    "                axs[count].set_title(f\"{yt}\")\n",
    "                axs[count].set_xlim((365,365*2-1))\n",
    "                axs[count].set_xticks(np.arange(365,365*2,60), np.arange(0,365,60)+1)\n",
    "                axs[count].set_ylabel('GPP')\n",
    "                \n",
    "                ## fill in SOS, EOS\n",
    "                df_SOS_EOS.loc[count, \"site_ID\"] = site_ID\n",
    "                df_SOS_EOS.loc[count, \"IGBP\"] = site_IGBP\n",
    "                df_SOS_EOS.loc[count, \"year\"] = int(yt)\n",
    "                df_SOS_EOS.loc[count, \"SOS_1_GPP\"] = SOS_EOS_DOY_GPP[0]-365\n",
    "                df_SOS_EOS.loc[count, \"EOS_1_GPP\"] = SOS_EOS_DOY_GPP[1]-365\n",
    "                df_SOS_EOS.loc[count, \"SOS_2_GPP\"] = SOS_EOS_DOY_GPP[2]-365\n",
    "                df_SOS_EOS.loc[count, \"EOS_2_GPP\"] = SOS_EOS_DOY_GPP[3]-365\n",
    "                \n",
    "                count = count + 1\n",
    "    \n",
    "                del grouped_df_concat3, smoothed_df_concat3, smoothed_df, scaled_df\n",
    "            \n",
    "            del site_data_daily, flux_df\n",
    "            # print(f\"get {a1} SOS and EOS\")\n",
    "            \n",
    "    \n",
    "            ## save figure\n",
    "            filename = f\"scaled_SOS_EOS_{site_ID}.png\"\n",
    "            # filename = f\"smooth_SOS_EOS_{site_ID}.png\"\n",
    "            fig.savefig(dir_savefig+filename, dpi=400, bbox_inches='tight')\n",
    "            print(f\"Saved png: {filename}\")\n",
    "            # plt.close()\n",
    "    \n",
    "            # --- Save filtered site data ---\n",
    "            fn_csv_out = f\"{site_ID}_SOS_EOS.csv\"\n",
    "            df_SOS_EOS.fillna(-9999).to_csv(dir_csv_out + fn_csv_out, index=False)\n",
    "            print(f\"Saved csv: {fn_csv_out}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sel_site}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ## Quality Control 3 - remove outliers\n",
    "        # site_data['time_year'] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "        # site_data['SW_IN_F'] = flux_df[\"SW_IN_F\"]\n",
    "        # for it, (var, qc) in enumerate(dic_QC_flux.items()):\n",
    "        #     # pick variable column and keep good or no qc flags data\n",
    "        #     df_var = flux_df[['TIMESTAMP_START', 'TIMESTAMP_END', 'SW_IN_F', var]].copy()\n",
    "        #     # copy Tair data for filtering freezing conditions\n",
    "        #     df_var['TA_low'] = flux_df['TA_F']\n",
    "        #     df_var['TA_low_QC'] = flux_df['TA_F_QC']\n",
    "        #     if df_var['TA_low'].notna().any():\n",
    "        #         df_var['TA_low'] = df_var['TA_low'].where(df_var['TA_low_QC'].isin([0,1]), np.nan)\n",
    "        #     else:\n",
    "        #         pass\n",
    "        #     df_var.drop('TA_low_QC', axis=1, inplace=True)\n",
    "\n",
    "        #     if var in ['GPP']:\n",
    "        #         print(f\"start outlier removing {site_ID} - {var}\")\n",
    "        #         # print(df_var.iloc[5760:5790, :])\n",
    "        #         df_var_no_outlier = remove_outlier(df_var, var)\n",
    "        #         df_var_no_outlier.drop(columns=['TA_low', 'hour_start'], inplace=True)\n",
    "        #         print(f\"finish outlier removing {site_ID} - {var}\")\n",
    "        #         # print(df_var_no_outlier.iloc[5760:5790, :])\n",
    "        #     else:\n",
    "        #         df_var_no_outlier = df_var.copy()\n",
    "        #         df_var_no_outlier.drop(columns=['TA_low'], inplace=True)\n",
    "\n",
    "        #     print(f\"complete {a1} QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "        #     # merge good data into dataframe\n",
    "        #     site_data = site_data.merge(df_var_no_outlier[['TIMESTAMP_START', 'TIMESTAMP_END', var]], on=['TIMESTAMP_START', 'TIMESTAMP_END'], how='left')\n",
    "        #     del df_var, df_var_no_outlier\n",
    "        #     # print(site_data.iloc[5760:5790, :])\n",
    "\n",
    "        # # print(site_data.columns)\n",
    "\n",
    "        # ## select time period: daytime (SW>10 W/m2)\n",
    "        # flux_time_mask = ((site_data[\"time_year\"] >= site_year_start) & \n",
    "        #         (site_data[\"time_year\"] <= site_year_end) & \n",
    "        #         (site_data[\"SW_IN_F\"]>=10))\n",
    "        # site_data = site_data[flux_time_mask]\n",
    "\n",
    "        # print(f\"complete {a1} selecting site_data\")\n",
    "\n",
    "        # ## daily mean\n",
    "        # site_data = site_data.set_index('TIMESTAMP_START')\n",
    "        # site_data_daily_tmp = site_data['GPP'].resample('D').mean()\n",
    "        # site_data_daily_tmp = site_data_daily_tmp.reset_index()\n",
    "        # site_data_daily_tmp = site_data_daily_tmp.rename(columns={\"TIMESTAMP_START\": \"date\"})\n",
    "        # site_data_daily = pd.DataFrame({'date': pd.date_range(start=f\"{int(site_year_start)}-01-01\", end=f\"{int(site_year_end)}-12-31\", freq='D')})\n",
    "        # site_data_daily = pd.merge(site_data_daily, site_data_daily_tmp, on='date', how='left')\n",
    "\n",
    "        # ## interpolation\n",
    "        # site_data_daily[\"GPP\"] = site_data_daily[\"GPP\"].interpolate(method='linear')\n",
    "        # site_data_daily[\"GPP\"] = site_data_daily[\"GPP\"].rolling(window_size, min_periods=5, center=True).apply(lambda x : np.nanmedian(x))\n",
    "        # site_data_daily.loc[np.isnan(site_data_daily[\"GPP\"]), \"GPP\"] = 0\n",
    "        # print(f\"complete {a1} interpolation and rolling mean\")\n",
    "\n",
    "        # ### --------------- get SOS, EOS every year ---------------\n",
    "        # site_data_daily[\"DOY\"] = site_data_daily[\"date\"].dt.dayofyear\n",
    "        # ## exclude all Feb 29 rows\n",
    "        # site_data_daily = site_data_daily[~((site_data_daily['date'].dt.month == 2) & (site_data_daily['date'].dt.day == 29))]\n",
    "    \n",
    "        # ## select year\n",
    "        # site_data_daily['Year'] = site_data_daily['date'].dt.year\n",
    "    \n",
    "        # ## create dataframe for SOS and EOS for each site\n",
    "        # df_SOS_EOS = pd.DataFrame()\n",
    "        \n",
    "        # ## smooth and scale data for each year\n",
    "        # site_years = site_data_daily.loc[len(site_data_daily)-1, 'Year'] - site_data_daily.loc[0, 'Year'] + 1\n",
    "        # site_year_start = int(site_data_daily.loc[0, 'Year'])\n",
    "        # site_year_end = int(site_data_daily.loc[len(site_data_daily)-1, 'Year'])\n",
    "        # count = 0\n",
    "        # fig, axs = plt.subplots(site_years, 1, figsize=(8,3*site_years))\n",
    "        # # for year, group in grouped_year:\n",
    "        # for yt in range(site_year_start, site_year_end+1, 1):\n",
    "        #     df_year_tmp = site_data_daily.copy()\n",
    "        #     df_year = df_year_tmp[df_year_tmp['Year'] == yt]\n",
    "            \n",
    "        #     ## smoothing grouped_year (use the middle concat result to avoid unsmoothed tail)\n",
    "        #     grouped_df_concat3 = pd.concat([df_year, df_year, df_year], axis=0)\n",
    "        #     grouped_df_concat3 = grouped_df_concat3.reset_index()\n",
    "    \n",
    "        #     smoothed_df_concat3 = grouped_df_concat3.copy()\n",
    "        #     for it in range(50):\n",
    "        #         smoothed_df_concat3 = smooth_savgol(smoothed_df_concat3, \"GPP_DT_VUT_REF\")\n",
    "        #     print(f\"complete {site_ID} smoothing year = {yt}\")\n",
    "\n",
    "        #     ## scale to [0,1]\n",
    "        #     smoothed_df = smoothed_df_concat3.copy()\n",
    "        #     columns_to_normalize = \"GPP\"\n",
    "        #     scaled_df = smoothed_df.copy()\n",
    "        #     scaler = MinMaxScaler()\n",
    "        #     scaled_df[columns_to_normalize] = scaler.fit_transform(smoothed_df[[columns_to_normalize]])\n",
    "        #     print(f\"complete {site_ID} scaling {yt}\")\n",
    "\n",
    "        #     ## get SOS, EOS\n",
    "        #     SOS_EOS_DOY_GPP, SOS_EOS_intersect_GPP, x_result_GPP = find_SOS_EOS(scaled_df, \"GPP\")\n",
    "\n",
    "        #     ## plot SOS, EOS\n",
    "        #     axs[count].plot(grouped_df_concat3.index.values, scaled_df[\"GPP\"])\n",
    "        #     print(f\"pass first line of figure {site_ID} - {yt}\")\n",
    "        #     for it in range(len(x_result_GPP)):\n",
    "        #         axs[count].axvline(x=x_result_GPP[it], color='black', linestyle='--')\n",
    "        #     for it in range(len(SOS_EOS_DOY_GPP)):\n",
    "        #         if (~np.isnan(SOS_EOS_DOY_GPP[it])):\n",
    "        #             axs[count].plot(SOS_EOS_DOY_GPP[it], SOS_EOS_intersect_GPP[it], 'o')\n",
    "        #         else:\n",
    "        #             pass\n",
    "        #     print(f\"pass second line of figure {site_ID} - {yt}\")\n",
    "        #     axs[count].set_title(f\"{year}\")\n",
    "        #     axs[count].set_xlim((365,365*2-1))\n",
    "        #     axs[count].set_xticks(np.arange(365,365*2,60), np.arange(0,365,60)+1)\n",
    "        #     axs[count].set_ylabel('GPP')\n",
    "            \n",
    "        #     ## fill in SOS, EOS\n",
    "        #     df_SOS_EOS.loc[count, \"site_ID\"] = site_ID\n",
    "        #     df_SOS_EOS.loc[count, \"IGBP\"] = site_IGBP\n",
    "        #     df_SOS_EOS.loc[count, \"year\"] = int(year)\n",
    "        #     df_SOS_EOS.loc[count, \"SOS_1_GPP\"] = SOS_EOS_DOY_GPP[0]-365\n",
    "        #     df_SOS_EOS.loc[count, \"EOS_1_GPP\"] = SOS_EOS_DOY_GPP[1]-365\n",
    "        #     df_SOS_EOS.loc[count, \"SOS_2_GPP\"] = SOS_EOS_DOY_GPP[2]-365\n",
    "        #     df_SOS_EOS.loc[count, \"EOS_2_GPP\"] = SOS_EOS_DOY_GPP[3]-365\n",
    "            \n",
    "        #     count = count + 1\n",
    "\n",
    "        #     del grouped_df_concat3, smoothed_df_concat3, smoothed_df, scaled_df\n",
    "            \n",
    "        # print(f\"get {a1} SOS and EOS\")\n",
    "\n",
    "        # ## save figure\n",
    "        # filename = f\"scaled_SOS_EOS_{site_ID}.png\"\n",
    "        # # filename = f\"smooth_SOS_EOS_{site_ID}.png\"\n",
    "        # fig.savefig(dir_savefig+filename, dpi=400, bbox_inches='tight')\n",
    "        # print(f\"Saved png: {filename}\")\n",
    "        # plt.close()\n",
    "    \n",
    "        # # --- Save filtered site data ---\n",
    "        # fn_csv_out = f\"{site_ID}_SOS_EOS.csv\"\n",
    "        # df_SOS_EOS.fillna(-9999).to_csv(dir_csv_out + fn_csv_out, index=False)\n",
    "        # print(f\"Saved csv: {fn_csv_out}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    elif (site_src == 'LBA-ECO'):\n",
    "        ## read site data\n",
    "        LBAECO_df = pd.read_excel(xlsx_dir+pattern, skiprows=[1])\n",
    "        print(f\"read site xlsx {site_ID}\")\n",
    "\n",
    "        # get date\n",
    "        LBAECO_df[\"combined\"] = LBAECO_df[\"Year_LBAMIP\"].astype(int)*1000 + LBAECO_df[\"DoY_LBAMIP\"].astype(int)\n",
    "        LBAECO_df[\"date\"] = pd.to_datetime(LBAECO_df[\"combined\"], format = \"%Y%j\")\n",
    "        LBAECO_df[\"datetime\"] = LBAECO_df[\"date\"] + pd.to_timedelta(LBAECO_df[\"Hour_LBAMIP\"], unit='h')\n",
    "\n",
    "        ## create a dataframe in filtering negative sensible heat flux\n",
    "        flux_df = pd.DataFrame(columns=pick_cols_LBAECO)\n",
    "\n",
    "        ## add site description and fill in data\n",
    "        flux_df[\"site_ID\"] = site_ID\n",
    "        flux_df[\"site_name\"] = site_name\n",
    "        flux_df[\"IGBP\"] = site_IGBP\n",
    "        flux_df[\"site_src\"] = site_src\n",
    "\n",
    "        flux_df[\"TIMESTAMP_END\"] = LBAECO_df[\"datetime\"]\n",
    "        flux_df[\"P_F\"] = LBAECO_df[\"prec\"] # mm\n",
    "        # flux_df[\"LE_flux\"] = LBAECO_df[\"LE\"]\n",
    "        # flux_df[\"H_flux\"] = LBAECO_df[\"H\"]\n",
    "        flux_df[\"TA_F\"] = LBAECO_df[\"ta\"] # C\n",
    "        # flux_df[\"PA_F\"] = LBAECO_df[\"press\"] # kPa\n",
    "        # flux_df[\"GPP_NT_VUT_REF\"] = LBAECO_df[\"GEP_model\"]\n",
    "        flux_df[\"GPP\"] = LBAECO_df[\"GEP_model\"]\n",
    "        # flux_df[\"VPD_F\"] = LBAECO_df[\"VPD\"] # kPa\n",
    "        flux_df[\"SW_IN_F\"] = LBAECO_df[\"rgs\"]\n",
    "        # flux_df[\"WS_F\"] = LBAECO_df[\"ws\"]\n",
    "        # flux_df[\"LW_IN_F\"] = LBAECO_df[\"rgl\"]\n",
    "        # flux_df[\"NETRAD\"] = LBAECO_df[\"Rn\"]\n",
    "        # flux_df[\"G_flux\"] = LBAECO_df[\"FG\"]\n",
    "        flux_df[\"USTAR\"] = LBAECO_df[\"ust\"]\n",
    "        del LBAECO_df\n",
    "\n",
    "        # ## convert unit\n",
    "        # flux_df[\"PA_F\"] = flux_df[\"PA_F\"]*10 # kPa -> hPa\n",
    "        # flux_df[\"VPD_F\"] = flux_df[\"VPD_F\"]*10 # kPa -> hPa\n",
    "\n",
    "        ## check whether data is half-hourly or hourly\n",
    "        if (flux_df[\"TIMESTAMP_END\"].iloc[1]-flux_df[\"TIMESTAMP_END\"].iloc[0] == pd.Timedelta(minutes=30)):\n",
    "            len_HH = 1\n",
    "            flux_df[\"TIMESTAMP_START\"] = flux_df[\"TIMESTAMP_END\"]-pd.Timedelta(minutes=30)\n",
    "        else:\n",
    "            len_HH = 0\n",
    "            flux_df[\"TIMESTAMP_START\"] = flux_df[\"TIMESTAMP_END\"]-pd.Timedelta(minutes=60)\n",
    "        print(\"site = \", site_ID, \", HH = \", len_HH) # HH=1 is half-hourly data\n",
    "        if  (len_HH == 1):\n",
    "            day_timestep = 48 # 48 timestep in a day\n",
    "            rain_QC_timestep = 24 # rain within 12 hours\n",
    "        else:\n",
    "            day_timestep = 24 # 24 timestep in a day\n",
    "            rain_QC_timestep = 12 # rain within 12 hours\n",
    "\n",
    "        ## Quality control 1\n",
    "        flux_df = flux_df.replace(-9999, np.nan)\n",
    "\n",
    "        # # 0<=RH<=100\n",
    "        # flux_df[\"e_sat\"] = es0*np.exp((Lv/Rv)*((1/T0)-(1/(flux_df[\"TA_F\"]+273.15))))\n",
    "        # flux_df[\"RH\"] = 100*(flux_df[\"e_sat\"]-flux_df[\"VPD_F\"])/flux_df[\"e_sat\"]\n",
    "        # flux_df[\"RH\"] = flux_df[\"RH\"].where((flux_df[\"RH\"]>=0) & (flux_df[\"RH\"]<=100))\n",
    "\n",
    "        # remove rain in previous 12 hours\n",
    "        flux_df.loc[len(flux_df)] = {col: np.nan for col in flux_df.columns} # add a nan row because df[\"var\"][a:b] does not count in df[\"var\"][b]\n",
    "        for pt in range(rain_QC_timestep*1, flux_df.shape[0]-1):\n",
    "            if ((np.nansum(flux_df.loc[pt-rain_QC_timestep:pt+1, \"P_F\"])==0) | (np.isnan(np.nansum(flux_df.loc[pt-rain_QC_timestep:pt+1, \"P_F\"])))):\n",
    "                pass\n",
    "            else:\n",
    "                flux_df.loc[pt, \"USTAR\"] = np.nan\n",
    "                flux_df.loc[pt, \"GPP\"] = np.nan\n",
    "\n",
    "        ## Get u* upper bound -> for Quality control 2\n",
    "        USTAR_mean = flux_df[\"USTAR\"].mean(axis=0)\n",
    "        USTAR_std = flux_df[\"USTAR\"].std(axis=0)\n",
    "        USTAR_upper = USTAR_mean+2*USTAR_std\n",
    "        flux_df.loc[((flux_df[\"USTAR\"]<=0.2) | (flux_df[\"USTAR\"]>USTAR_upper)), 'GPP'] = np.nan\n",
    "        print(f\"complete {site_ID} QC 2 (exclude too low and too high USTAR for fluxes)\")\n",
    "        # print(flux_df.loc[5760:5790, \"GPP_DT_VUT_REF\"])\n",
    "\n",
    "        # ## Quality Control 3 - remove outliers\n",
    "        # site_data['time_year'] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "        # site_data['SW_IN_F'] = flux_df[\"SW_IN_F\"]\n",
    "        # for it, (var, qc) in enumerate(dic_QC_flux.items()):\n",
    "        #     # pick variable column and keep good or no qc flags data\n",
    "        #     df_var = flux_df[['TIMESTAMP_START', 'TIMESTAMP_END', 'SW_IN_F', var]].copy()\n",
    "        #     # copy Tair data for filtering freezing conditions\n",
    "        #     df_var['TA_low'] = flux_df['TA_F']\n",
    "        #     df_var['TA_low_QC'] = flux_df['TA_F_QC']\n",
    "        #     if df_var['TA_low'].notna().any():\n",
    "        #         df_var['TA_low'] = df_var['TA_low'].where(df_var['TA_low_QC'].isin([0,1]), np.nan)\n",
    "        #     else:\n",
    "        #         pass\n",
    "        #     df_var.drop('TA_low_QC', axis=1, inplace=True)\n",
    "\n",
    "        #     if var in ['GPP']:\n",
    "        #         print(f\"start outlier removing {site_ID} - {var}\")\n",
    "        #         # print(df_var.iloc[5760:5790, :])\n",
    "        #         df_var_no_outlier = remove_outlier(df_var, var)\n",
    "        #         df_var_no_outlier.drop(columns=['TA_low', 'hour_start'], inplace=True)\n",
    "        #         print(f\"finish outlier removing {site_ID} - {var}\")\n",
    "        #         # print(df_var_no_outlier.iloc[5760:5790, :])\n",
    "        #     else:\n",
    "        #         df_var_no_outlier = df_var.copy()\n",
    "        #         df_var_no_outlier.drop(columns=['TA_low'], inplace=True)\n",
    "\n",
    "        #     print(f\"complete {a1} QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "        #     # merge good data into dataframe\n",
    "        #     site_data = site_data.merge(df_var_no_outlier[['TIMESTAMP_START', 'TIMESTAMP_END', var]], on=['TIMESTAMP_START', 'TIMESTAMP_END'], how='left')\n",
    "        #     del df_var, df_var_no_outlier\n",
    "\n",
    "        # # --- Quality Control 3 - remove outliers (robust) ---\n",
    "        # required_base = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"SW_IN_F\"]\n",
    "        # missing_base = [c for c in required_base if c not in flux_df.columns]\n",
    "        # if missing_base:\n",
    "        #     raise KeyError(f\"[{site_ID}] Missing base columns before QC3: {missing_base}\")\n",
    "        \n",
    "        # # ensure site_data exists and aligns\n",
    "        # site_data = pd.DataFrame({\n",
    "        #     \"TIMESTAMP_START\": flux_df[\"TIMESTAMP_START\"],\n",
    "        #     \"TIMESTAMP_END\": flux_df[\"TIMESTAMP_END\"],\n",
    "        # })\n",
    "        # site_data[\"time_year\"] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "        # site_data[\"SW_IN_F\"]   = flux_df[\"SW_IN_F\"]\n",
    "        \n",
    "        # # bring the dict into scope robustly (works even in child processes)\n",
    "        # try:\n",
    "        #     _qc_map = dic_QC_Ozflux_flux\n",
    "        # except NameError:\n",
    "        #     # fallback: minimal map for GPP\n",
    "        #     _qc_map = {\"GPP\": \"GPP_QC\"}\n",
    "        #     print(f\"[{site_ID}] dic_QC_Ozflux_flux not found in worker; using fallback: {_qc_map}\")\n",
    "        \n",
    "        # if not isinstance(_qc_map, dict) or not _qc_map:\n",
    "        #     print(f\"[{site_ID}] QC map empty or not a dict: {_qc_map} — skipping QC3 loop.\")\n",
    "        # else:\n",
    "        #     # keep only variables that exist\n",
    "        #     vars_present = [v for v in _qc_map.keys() if v in flux_df.columns]\n",
    "        #     if not vars_present:\n",
    "        #         print(f\"[{site_ID}] None of {_qc_map.keys()} present in flux_df; skipping QC3 loop.\")\n",
    "        #     else:\n",
    "        #         print(f\"[{site_ID}] QC3 will process variables: {vars_present}\")\n",
    "\n",
    "        #     for it, var in enumerate(vars_present):\n",
    "        #         qc_col = _qc_map[var]\n",
    "        #         need_cols = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"SW_IN_F\", var]\n",
    "        #         missing_now = [c for c in need_cols if c not in flux_df.columns]\n",
    "        #         if missing_now:\n",
    "        #             print(f\"[{site_ID}] Skipping {var}: missing columns {missing_now}\")\n",
    "        #             continue\n",
    "        #         if qc_col not in flux_df.columns:\n",
    "        #             print(f\"[{site_ID}] Skipping {var}: QC column '{qc_col}' not found.\")\n",
    "        #             continue\n",
    "        \n",
    "        #         df_var = flux_df[need_cols].copy()\n",
    "        #         # Optional Tair screen if present\n",
    "        #         if (\"TA_F\" in flux_df.columns) and (\"TA_F_QC\" in flux_df.columns):\n",
    "        #             df_var[\"TA_low\"] = flux_df[\"TA_F\"].where(flux_df[\"TA_F_QC\"].isin([0, 10, 20]), np.nan)\n",
    "        \n",
    "        #         if var in [\"GPP\"]:\n",
    "        #             print(f\"[{site_ID}] start outlier removing - {var} (rows={len(df_var)})\")\n",
    "        #             df_var_no_outlier = remove_outlier(df_var, var)\n",
    "        #             df_var_no_outlier.drop(columns=[\"TA_low\", \"hour_start\"], inplace=True, errors=\"ignore\")\n",
    "        #             print(f\"[{site_ID}] finish outlier removing - {var}\")\n",
    "        #         else:\n",
    "        #             df_var_no_outlier = df_var.copy()\n",
    "        #             df_var_no_outlier.drop(columns=[\"TA_low\"], inplace=True, errors=\"ignore\")\n",
    "        \n",
    "        #         # Merge back\n",
    "        #         site_data = site_data.merge(\n",
    "        #             df_var_no_outlier[[\"TIMESTAMP_START\", \"TIMESTAMP_END\", var]],\n",
    "        #             on=[\"TIMESTAMP_START\", \"TIMESTAMP_END\"], how=\"left\"\n",
    "        #         )\n",
    "        #         del df_var, df_var_no_outlier\n",
    "        \n",
    "        #     print(f\"[{site_ID}] complete QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "        # -------------------------\n",
    "        # QC3: Outlier removal (QC-free)\n",
    "        # -------------------------\n",
    "        required_base = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"SW_IN_F\"]\n",
    "        missing_base  = [c for c in required_base if c not in flux_df.columns]\n",
    "        if missing_base:\n",
    "            raise KeyError(f\"[{site_ID}] Missing base columns before QC3: {missing_base}\")\n",
    "        \n",
    "        # Build site_data aligned with flux_df (no QC dict; just work on GPP if present)\n",
    "        site_data = pd.DataFrame({\n",
    "            \"TIMESTAMP_START\": flux_df[\"TIMESTAMP_START\"],\n",
    "            \"TIMESTAMP_END\"  : flux_df[\"TIMESTAMP_END\"],\n",
    "        })\n",
    "        site_data[\"time_year\"] = flux_df[\"TIMESTAMP_START\"].dt.year\n",
    "        site_data[\"SW_IN_F\"]   = flux_df[\"SW_IN_F\"]\n",
    "        \n",
    "        # Choose variables to outlier-filter (no QC map). Here: GPP only if available.\n",
    "        vars_present = [v for v in [\"GPP\"] if v in flux_df.columns]\n",
    "        \n",
    "        for var in vars_present:\n",
    "            need_cols  = [\"TIMESTAMP_START\", \"TIMESTAMP_END\", \"SW_IN_F\", var]\n",
    "            missing_now = [c for c in need_cols if c not in flux_df.columns]\n",
    "            if missing_now:\n",
    "                print(f\"[{site_ID}] Skipping {var}: missing columns {missing_now}\")\n",
    "                continue\n",
    "        \n",
    "            df_var = flux_df[need_cols].copy()\n",
    "        \n",
    "            # Run your outlier remover (assumes it can work without QC flags)\n",
    "            print(f\"[{site_ID}] start outlier removing - {var} (rows={len(df_var)})\")\n",
    "            df_var_no_outlier = remove_outlier(df_var, var)\n",
    "            # Some versions add 'hour_start'; drop if present. No TA_low used here.\n",
    "            df_var_no_outlier.drop(columns=[\"hour_start\"], inplace=True, errors=\"ignore\")\n",
    "            print(f\"[{site_ID}] finish outlier removing - {var}\")\n",
    "        \n",
    "            # Merge back\n",
    "            site_data = site_data.merge(\n",
    "                df_var_no_outlier[[\"TIMESTAMP_START\", \"TIMESTAMP_END\", var]],\n",
    "                on=[\"TIMESTAMP_START\", \"TIMESTAMP_END\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "            del df_var, df_var_no_outlier\n",
    "        \n",
    "        print(f\"[{site_ID}] complete QC 3 (remove outlier for fluxes)\")\n",
    "\n",
    "        ## select time period: daytime (SW>10 W/m2)\n",
    "        flux_time_mask = ((site_data[\"time_year\"] >= site_year_start) & \n",
    "                (site_data[\"time_year\"] <= site_year_end) & \n",
    "                (site_data[\"SW_IN_F\"]>=50))\n",
    "        site_data = site_data[flux_time_mask]\n",
    "\n",
    "        print(f\"complete {site_ID} selecting site_data\")\n",
    "\n",
    "        ## daily mean\n",
    "        site_data = site_data.set_index('TIMESTAMP_START')\n",
    "        site_data_daily_tmp = site_data['GPP'].resample('D').mean()\n",
    "        site_data_daily_tmp = site_data_daily_tmp.reset_index()\n",
    "        site_data_daily_tmp = site_data_daily_tmp.rename(columns={\"TIMESTAMP_START\": \"date\"})\n",
    "        site_data_daily = pd.DataFrame({'date': pd.date_range(start=f\"{int(site_year_start)}-01-01\", end=f\"{int(site_year_end)}-12-31\", freq='D')})\n",
    "        site_data_daily = pd.merge(site_data_daily, site_data_daily_tmp, on='date', how='left')\n",
    "\n",
    "        ## interpolation\n",
    "        site_data_daily[\"GPP\"] = site_data_daily[\"GPP\"].interpolate(method='linear')\n",
    "        site_data_daily[\"GPP\"] = site_data_daily[\"GPP\"].rolling(window_size, min_periods=5, center=True).apply(lambda x : np.nanmedian(x))\n",
    "        site_data_daily.loc[np.isnan(site_data_daily[\"GPP\"]), \"GPP\"] = 0\n",
    "        print(f\"complete {site_ID} interpolation and rolling mean\")\n",
    "\n",
    "                ### --------------- get SOS, EOS every year ---------------\n",
    "        site_data_daily[\"DOY\"] = site_data_daily[\"date\"].dt.dayofyear\n",
    "        ## exclude all Feb 29 rows\n",
    "        site_data_daily = site_data_daily[~((site_data_daily['date'].dt.month == 2) & (site_data_daily['date'].dt.day == 29))]\n",
    "    \n",
    "        ## select year\n",
    "        site_data_daily['Year'] = site_data_daily['date'].dt.year\n",
    "    \n",
    "        ## create dataframe for SOS and EOS for each site\n",
    "        df_SOS_EOS = pd.DataFrame()\n",
    "        \n",
    "        ## smooth and scale data for each year\n",
    "        site_years = site_data_daily.loc[len(site_data_daily)-1, 'Year'] - site_data_daily.loc[0, 'Year'] + 1\n",
    "        site_year_start = int(site_data_daily.loc[0, 'Year'])\n",
    "        site_year_end = int(site_data_daily.loc[len(site_data_daily)-1, 'Year'])\n",
    "        count = 0\n",
    "        fig, axs = plt.subplots(site_years, 1, figsize=(8,3*site_years))\n",
    "        # for year, group in grouped_year:\n",
    "        for yt in range(site_year_start, site_year_end+1, 1):\n",
    "            df_year_tmp = site_data_daily.copy()\n",
    "            df_year = df_year_tmp[df_year_tmp['Year'] == yt]\n",
    "            \n",
    "            ## smoothing grouped_year (use the middle concat result to avoid unsmoothed tail)\n",
    "            grouped_df_concat3 = pd.concat([df_year, df_year, df_year], axis=0)\n",
    "            grouped_df_concat3 = grouped_df_concat3.reset_index()\n",
    "    \n",
    "            smoothed_df_concat3 = grouped_df_concat3.copy()\n",
    "            for it in range(50):\n",
    "                smoothed_df_concat3 = smooth_savgol(smoothed_df_concat3, \"GPP\")\n",
    "            print(f\"complete {site_ID} smoothing year = {yt}\")\n",
    "\n",
    "            ## scale to [0,1]\n",
    "            smoothed_df = smoothed_df_concat3.copy()\n",
    "            columns_to_normalize = \"GPP\"\n",
    "            scaled_df = smoothed_df.copy()\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_df[columns_to_normalize] = scaler.fit_transform(smoothed_df[[columns_to_normalize]])\n",
    "            print(f\"complete {site_ID} scaling {yt}\")\n",
    "\n",
    "            ## get SOS, EOS\n",
    "            SOS_EOS_DOY_GPP, SOS_EOS_intersect_GPP, x_result_GPP = find_SOS_EOS(scaled_df, \"GPP\")\n",
    "\n",
    "            ## plot SOS, EOS\n",
    "            axs[count].plot(grouped_df_concat3.index.values, scaled_df[\"GPP\"])\n",
    "            print(f\"pass first line of figure {site_ID} - {yt}\")\n",
    "            for it in range(len(x_result_GPP)):\n",
    "                axs[count].axvline(x=x_result_GPP[it], color='black', linestyle='--')\n",
    "            for it in range(len(SOS_EOS_DOY_GPP)):\n",
    "                if (~np.isnan(SOS_EOS_DOY_GPP[it])):\n",
    "                    axs[count].plot(SOS_EOS_DOY_GPP[it], SOS_EOS_intersect_GPP[it], 'o')\n",
    "                else:\n",
    "                    pass\n",
    "            print(f\"pass second line of figure {site_ID} - {yt}\")\n",
    "            axs[count].set_title(f\"{yt}\")\n",
    "            axs[count].set_xlim((365,365*2-1))\n",
    "            axs[count].set_xticks(np.arange(365,365*2,60), np.arange(0,365,60)+1)\n",
    "            axs[count].set_ylabel('GPP')\n",
    "            \n",
    "            ## fill in SOS, EOS\n",
    "            df_SOS_EOS.loc[count, \"site_ID\"] = site_ID\n",
    "            df_SOS_EOS.loc[count, \"IGBP\"] = site_IGBP\n",
    "            df_SOS_EOS.loc[count, \"year\"] = int(yt)\n",
    "            df_SOS_EOS.loc[count, \"SOS_1_GPP\"] = SOS_EOS_DOY_GPP[0]-365\n",
    "            df_SOS_EOS.loc[count, \"EOS_1_GPP\"] = SOS_EOS_DOY_GPP[1]-365\n",
    "            df_SOS_EOS.loc[count, \"SOS_2_GPP\"] = SOS_EOS_DOY_GPP[2]-365\n",
    "            df_SOS_EOS.loc[count, \"EOS_2_GPP\"] = SOS_EOS_DOY_GPP[3]-365\n",
    "            \n",
    "            count = count + 1\n",
    "\n",
    "            del grouped_df_concat3, smoothed_df_concat3, smoothed_df, scaled_df\n",
    "            \n",
    "        print(f\"get {site_ID} SOS and EOS\")\n",
    "\n",
    "        ## save figure\n",
    "        filename = f\"scaled_SOS_EOS_{site_ID}.png\"\n",
    "        # filename = f\"smooth_SOS_EOS_{site_ID}.png\"\n",
    "        fig.savefig(dir_savefig+filename, dpi=400, bbox_inches='tight')\n",
    "        print(f\"Saved png: {filename}\")\n",
    "        plt.close()\n",
    "    \n",
    "        # --- Save filtered site data ---\n",
    "        fn_csv_out = f\"{site_ID}_SOS_EOS.csv\"\n",
    "        df_SOS_EOS.fillna(-9999).to_csv(dir_csv_out + fn_csv_out, index=False)\n",
    "        print(f\"Saved csv: {fn_csv_out}\")\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # shutil.rmtree(\"/burg/glab/users/rg3390/data/FLUXNET2015/ICOS/warm_winter_2020_release_2022/tmp_unzip/\")\n",
    "    # shutil.rmtree(\"/burg/glab/users/rg3390/data/FLUXNET2015/AmeriFlux/tmp_unzip/\")\n",
    "    # shutil.rmtree(\"/burg/glab/users/rg3390/data/FLUXNET2015/FLUXNET2015_zipped/tmp_unzip/\")\n",
    "\n",
    "\n",
    "    return None\n",
    "        \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97c1ac55-2a9d-48dc-ac1f-465b5f48a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append all 158 sites!\n",
      "in process_and_plot K67 and source from LBA-ECOin process_and_plot K34 and source from LBA-ECO"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sites:   0%|                                                                                          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in process_and_plot BAN and source from LBA-ECOin process_and_plot CAX and source from LBA-ECOin process_and_plot RJA and source from LBA-ECOin process_and_plot K83 and source from LBA-ECO\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "choosing directory for K34 and source from LBA-ECOchoosing directory for BAN and source from LBA-ECOchoosing directory for K67 and source from LBA-ECOchoosing directory for CAX and source from LBA-ECOchoosing directory for RJA and source from LBA-ECOchoosing directory for K83 and source from LBA-ECO\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "read site xlsx K67\n",
      "read site xlsx RJA\n",
      "site = site =   K67RJA  , HH = , HH =   00\n",
      "\n",
      "read site xlsx BAN\n",
      "site =  BAN , HH =  0\n",
      "complete BAN QC 2 (exclude too low and too high USTAR for fluxes)\n",
      "[BAN] start outlier removing - GPP (rows=35041)\n",
      "complete K67 QC 2 (exclude too low and too high USTAR for fluxes)complete RJA QC 2 (exclude too low and too high USTAR for fluxes)\n",
      "\n",
      "[K67] start outlier removing - GPP (rows=35041)[RJA] start outlier removing - GPP (rows=35041)\n",
      "\n",
      "read site xlsx K83\n",
      "site =  K83 , HH =  0\n",
      "read site xlsx CAX\n",
      "site =  CAX , HH =  0\n",
      "complete K83 QC 2 (exclude too low and too high USTAR for fluxes)\n",
      "[K83] start outlier removing - GPP (rows=43801)\n",
      "complete CAX QC 2 (exclude too low and too high USTAR for fluxes)\n",
      "[CAX] start outlier removing - GPP (rows=43801)\n",
      "read site xlsx K34\n",
      "site =  K34 , HH =  0\n",
      "[BAN] finish outlier removing - GPP\n",
      "[BAN] complete QC 3 (remove outlier for fluxes)\n",
      "complete BAN selecting site_data\n",
      "complete BAN interpolation and rolling mean\n",
      "complete BAN smoothing year = 2003\n",
      "complete BAN scaling 2003\n",
      "pass first line of figure BAN - 2003\n",
      "pass second line of figure BAN - 2003\n",
      "complete BAN smoothing year = 2004\n",
      "complete BAN scaling 2004\n",
      "pass first line of figure BAN - 2004\n",
      "pass second line of figure BAN - 2004\n",
      "complete BAN smoothing year = 2005\n",
      "complete BAN scaling 2005\n",
      "pass first line of figure BAN - 2005\n",
      "pass second line of figure BAN - 2005\n",
      "complete BAN smoothing year = 2006\n",
      "complete BAN scaling 2006\n",
      "pass first line of figure BAN - 2006\n",
      "pass second line of figure BAN - 2006\n",
      "get BAN SOS and EOS\n",
      "Saved png: scaled_SOS_EOS_BAN.png\n",
      "Saved csv: BAN_SOS_EOS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sites:  17%|█████████████▌                                                                   | 1/6 [01:55<09:37, 115.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RJA] finish outlier removing - GPP\n",
      "[RJA] complete QC 3 (remove outlier for fluxes)\n",
      "complete RJA selecting site_data\n",
      "complete RJA interpolation and rolling mean\n",
      "complete RJA smoothing year = 1999\n",
      "complete RJA scaling 1999\n",
      "pass first line of figure RJA - 1999\n",
      "pass second line of figure RJA - 1999\n",
      "complete RJA smoothing year = 2000\n",
      "complete RJA scaling 2000\n",
      "pass first line of figure RJA - 2000\n",
      "pass second line of figure RJA - 2000\n",
      "complete RJA smoothing year = 2001\n",
      "complete RJA scaling 2001\n",
      "pass first line of figure RJA - 2001\n",
      "pass second line of figure RJA - 2001\n",
      "complete RJA smoothing year = 2002\n",
      "complete RJA scaling 2002\n",
      "pass first line of figure RJA - 2002\n",
      "pass second line of figure RJA - 2002\n",
      "get RJA SOS and EOS\n",
      "Saved png: scaled_SOS_EOS_RJA.png\n",
      "Saved csv: RJA_SOS_EOS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sites:  33%|███████████████████████████▎                                                      | 2/6 [01:58<03:16, 49.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K67] finish outlier removing - GPP\n",
      "[K67] complete QC 3 (remove outlier for fluxes)\n",
      "complete K67 selecting site_data\n",
      "complete K67 interpolation and rolling mean\n",
      "complete K67 smoothing year = 2002\n",
      "complete K67 scaling 2002\n",
      "pass first line of figure K67 - 2002\n",
      "pass second line of figure K67 - 2002\n",
      "complete K67 smoothing year = 2003\n",
      "complete K67 scaling 2003\n",
      "pass first line of figure K67 - 2003\n",
      "pass second line of figure K67 - 2003\n",
      "complete K67 smoothing year = 2004\n",
      "complete K67 scaling 2004\n",
      "pass first line of figure K67 - 2004\n",
      "pass second line of figure K67 - 2004\n",
      "complete K67 smoothing year = 2005\n",
      "complete K67 scaling 2005\n",
      "pass first line of figure K67 - 2005\n",
      "pass second line of figure K67 - 2005\n",
      "complete K67 smoothing year = 2006\n",
      "complete K67 scaling 2006\n",
      "pass first line of figure K67 - 2006\n",
      "pass second line of figure K67 - 2006\n",
      "get K67 SOS and EOS\n",
      "Saved png: scaled_SOS_EOS_K67.png\n",
      "Saved csv: K67_SOS_EOS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sites:  50%|█████████████████████████████████████████                                         | 3/6 [02:03<01:27, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete K34 QC 2 (exclude too low and too high USTAR for fluxes)\n",
      "[K34] start outlier removing - GPP (rows=70081)\n",
      "[K83] finish outlier removing - GPP\n",
      "[CAX] finish outlier removing - GPP\n",
      "[K83] complete QC 3 (remove outlier for fluxes)\n",
      "complete K83 selecting site_data\n",
      "[CAX] complete QC 3 (remove outlier for fluxes)\n",
      "complete CAX selecting site_data\n",
      "complete K83 interpolation and rolling mean\n",
      "complete CAX interpolation and rolling mean\n",
      "complete K83 smoothing year = 2000\n",
      "complete K83 scaling 2000\n",
      "complete CAX smoothing year = 1999\n",
      "complete CAX scaling 1999\n",
      "pass first line of figure CAX - 1999\n",
      "pass first line of figure K83 - 2000\n",
      "pass second line of figure CAX - 1999\n",
      "pass second line of figure K83 - 2000\n",
      "complete CAX smoothing year = 2000\n",
      "complete K83 smoothing year = 2001\n",
      "complete CAX scaling 2000\n",
      "complete K83 scaling 2001\n",
      "pass first line of figure CAX - 2000\n",
      "pass second line of figure CAX - 2000\n",
      "pass first line of figure K83 - 2001\n",
      "pass second line of figure K83 - 2001\n",
      "complete CAX smoothing year = 2001\n",
      "complete CAX scaling 2001\n",
      "complete K83 smoothing year = 2002\n",
      "complete K83 scaling 2002\n",
      "pass first line of figure CAX - 2001\n",
      "pass second line of figure CAX - 2001\n",
      "complete CAX smoothing year = 2002\n",
      "complete CAX scaling 2002\n",
      "pass first line of figure K83 - 2002\n",
      "pass second line of figure K83 - 2002\n",
      "complete K83 smoothing year = 2003\n",
      "complete K83 scaling 2003\n",
      "pass first line of figure K83 - 2003\n",
      "pass first line of figure CAX - 2002\n",
      "pass second line of figure K83 - 2003\n",
      "pass second line of figure CAX - 2002\n",
      "complete K83 smoothing year = 2004\n",
      "complete K83 scaling 2004\n",
      "complete CAX smoothing year = 2003\n",
      "complete CAX scaling 2003\n",
      "pass first line of figure CAX - 2003\n",
      "pass second line of figure CAX - 2003\n",
      "get CAX SOS and EOS\n",
      "pass first line of figure K83 - 2004\n",
      "pass second line of figure K83 - 2004\n",
      "get K83 SOS and EOS\n",
      "Saved png: scaled_SOS_EOS_CAX.png\n",
      "Saved csv: CAX_SOS_EOS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sites:  67%|██████████████████████████████████████████████████████▋                           | 4/6 [02:37<01:01, 30.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved png: scaled_SOS_EOS_K83.png\n",
      "Saved csv: K83_SOS_EOS.csv\n",
      "[K34] finish outlier removing - GPP\n",
      "[K34] complete QC 3 (remove outlier for fluxes)\n",
      "complete K34 selecting site_data\n",
      "complete K34 interpolation and rolling mean\n",
      "complete K34 smoothing year = 1999\n",
      "complete K34 scaling 1999\n",
      "pass first line of figure K34 - 1999\n",
      "pass second line of figure K34 - 1999\n",
      "complete K34 smoothing year = 2000\n",
      "complete K34 scaling 2000\n",
      "pass first line of figure K34 - 2000\n",
      "pass second line of figure K34 - 2000\n",
      "complete K34 smoothing year = 2001\n",
      "complete K34 scaling 2001\n",
      "pass first line of figure K34 - 2001\n",
      "pass second line of figure K34 - 2001\n",
      "complete K34 smoothing year = 2002\n",
      "complete K34 scaling 2002\n",
      "pass first line of figure K34 - 2002\n",
      "pass second line of figure K34 - 2002\n",
      "complete K34 smoothing year = 2003\n",
      "complete K34 scaling 2003\n",
      "pass first line of figure K34 - 2003\n",
      "pass second line of figure K34 - 2003\n",
      "complete K34 smoothing year = 2004\n",
      "complete K34 scaling 2004\n",
      "pass first line of figure K34 - 2004\n",
      "pass second line of figure K34 - 2004\n",
      "complete K34 smoothing year = 2005\n",
      "complete K34 scaling 2005\n",
      "pass first line of figure K34 - 2005\n",
      "pass second line of figure K34 - 2005\n",
      "complete K34 smoothing year = 2006\n",
      "complete K34 scaling 2006\n",
      "pass first line of figure K34 - 2006\n",
      "pass second line of figure K34 - 2006\n",
      "get K34 SOS and EOS\n",
      "Saved png: scaled_SOS_EOS_K34.png\n",
      "Saved csv: K34_SOS_EOS.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sites: 100%|██████████████████████████████████████████████████████████████████████████████████| 6/6 [04:43<00:00, 47.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# ------------ Run processing in parallel ------------ \n",
    "if __name__ == \"__main__\":\n",
    "    # site_info_df = pd.DataFrame(columns = ['site_ID', 'site_year_start', 'site_year_end', 'filepath', 'site_IGBP', 'dic_varQC_first', 'dic_varQC', 'dir_save_fig', 'dir_csv_out', 'pick_variables_rename', 'pick_variables_unit_precip_hh', 'pick_variables_unit_precip_hr'])\n",
    "    site_info_list = []\n",
    "    for st in range(152, 158): # total_site\n",
    "        site_src = df_forest_sitelist[\"source\"].iloc[st] # site source\n",
    "        site_ID = df_forest_sitelist[\"site_ID\"].iloc[st]\n",
    "        site_name = df_forest_sitelist[\"SITE_NAME\"].iloc[st]\n",
    "        site_IGBP = df_forest_sitelist[\"IGBP\"].iloc[st]\n",
    "        site_year_start = df_forest_sitelist[\"year_start\"].iloc[st]\n",
    "        site_year_end = df_forest_sitelist[\"year_end\"].iloc[st]\n",
    "        # print(\"start listing\", site_ID)\n",
    "\n",
    "        site_info_list.append((\n",
    "                            site_ID,\n",
    "                            site_name,\n",
    "                            int(site_year_start),\n",
    "                            int(site_year_end),\n",
    "                            site_IGBP,\n",
    "                            site_src\n",
    "                        ))\n",
    "                        \n",
    "    print(f\"append all {st+1} sites!\")\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_and_plot, args): args[0] for args in site_info_list}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing sites\"):\n",
    "            pass\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc73f4-1472-4df5-9940-6e5778687d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc27dd1-4029-4664-b835-cb88b5ffb88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86d0e4-657c-4007-9450-e9991725010b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7be1f17-cec0-4e6b-a499-9d39dab13f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AU-Wrr', 'Warra', 2013, 2021, 'MF', 'Ozflux'),\n",
       " ('BE-Bra', 'Brasschaat', 1996, 2020, 'MF', 'ICOS'),\n",
       " ('BE-Vie', 'Vielsalm', 1996, 2020, 'MF', 'ICOS'),\n",
       " ('BR-Sa1', 'Santarem-Km67-Primary Forest', 2002, 2011, 'EBF', 'FLUXNET'),\n",
       " ('BR-Sa3', 'Santarem-Km83-Logged Forest', 2000, 2004, 'EBF', 'FLUXNET')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816e7050-94e5-41b1-995a-089c06888c52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_SOS_EOS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_SOS_EOS\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_SOS_EOS' is not defined"
     ]
    }
   ],
   "source": [
    "df_SOS_EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7bf08-cb2a-4599-b977-4ab5a8537148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
